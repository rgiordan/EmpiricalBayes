#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbe}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Part
Standard Empirical Bayes
\end_layout

\begin_layout Section
Model and problem statement
\end_layout

\begin_layout Standard
Given a parameter 
\begin_inset Formula $\theta$
\end_inset

, a meta-parameter 
\begin_inset Formula $\alpha$
\end_inset

, and data 
\begin_inset Formula $x$
\end_inset

, we want the posterior:
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta,\alpha\vert x\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)d\theta d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We will do empirical Bayes on 
\begin_inset Formula $\alpha$
\end_inset

 and MCMC on 
\begin_inset Formula $\theta$
\end_inset

.
 Specifically, given
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha} & := & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)d\theta\right\} \\
\theta_{n} & \sim & p\left(\theta\vert x,\hat{\alpha}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
A common question is how uncertainty in 
\begin_inset Formula $\alpha$
\end_inset

 would translate into uncertainty in 
\begin_inset Formula $\theta$
\end_inset

 if it were not fixed at 
\begin_inset Formula $\hat{\alpha}$
\end_inset

.
 We can answer this question with linear response and importance sampling
 by conditioning on the MCMC sample.
\end_layout

\begin_layout Section
Derivation
\end_layout

\begin_layout Standard
Suppose we have a tilted posterior
\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(\theta,\alpha\vert x\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}\\
 & = & \exp\left(\ell\left(\theta,\alpha,t\right)-c\left(t\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where for notational convenience we have written
\begin_inset Formula 
\begin{eqnarray*}
\ell\left(\theta,\alpha,t\right) & = & \log\left(p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}\right)\\
c\left(t\right) & = & \log\left(\int\exp\left(\ell\left(\theta,\alpha,t\right)\right)d\theta d\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
with implicit dependence on 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

.
 Then
\begin_inset Formula 
\begin{eqnarray*}
\Sigma_{\theta}:=\frac{d}{dt^{T}}\mbe_{p_{t}}\left[\theta\vert x\right] & = & Cov_{p}\left(\theta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Under the assumption that the Empirical Bayes approximation is good, then
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{p_{t}}\left[\theta\vert x\right] & \approx & \mbe_{p_{t}}\left[\theta\vert x,\alpha\right]\Rightarrow\\
\hat{\Sigma}_{\theta} & := & \frac{d}{dt^{T}}\mbe_{p_{t}}\left[\theta\vert x,\alpha\right]\approx\frac{d}{dt^{T}}\mbe_{p_{t}}\left[\theta\vert x\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\alpha$
\end_inset

 depends on 
\begin_inset Formula $t$
\end_inset

 since, as a function of 
\begin_inset Formula $t$
\end_inset

, we have
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & := & \underset{\alpha}{\textrm{argmax}}\left\{ \int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}d\theta\right\} \\
\hat{\alpha} & = & \hat{\alpha}_{0}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note at this point that 
\begin_inset Formula $\hat{\alpha}_{t}$
\end_inset

 is a function of 
\begin_inset Formula $t$
\end_inset

, and 
\begin_inset Formula $\mbe_{p_{t}}\left[\theta\vert x,\hat{\alpha}_{t}\right]$
\end_inset

 is a function of both 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

.
 Taking the total derivative,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \left.\frac{\partial\mbe_{p_{t}}\left[\theta\vert x,\alpha\right]}{\partial\alpha^{T}}\frac{d\alpha}{dt^{T}}\right|_{\alpha=\hat{\alpha}_{0},t=0}+\left.\frac{\partial\mbe_{p_{t}}\left[\theta\vert x,\hat{\alpha}_{0}\right]}{\partial t}\right|_{t=0}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In order to evaluate 
\begin_inset Formula $\frac{d}{dt^{T}}\mbe_{p_{t}}\left[\theta\right]$
\end_inset

 we will need two terms: 
\begin_inset Formula $\frac{d}{d\alpha^{T}}\mbe_{p_{t}}\left[\theta\vert x,\alpha\right]$
\end_inset

 and 
\begin_inset Formula $\frac{d\alpha}{dt}$
\end_inset

.
 Note that 
\begin_inset Formula $\frac{\partial\mbe_{p_{t}}\left[\theta\vert x,\alpha\right]}{\partial t}=\textrm{Var}\left(\theta\vert x,\alpha\right)$
\end_inset

, so this has the form of the conditional variance plus a correction term.
\end_layout

\begin_layout Subsection
\begin_inset Formula $d\alpha/dt$
\end_inset


\end_layout

\begin_layout Standard
First, we will calculate 
\begin_inset Formula $d\alpha/dt$
\end_inset

.
 Recall that
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & := & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}d\theta\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Let us define 
\begin_inset Formula 
\begin{eqnarray*}
M\left(\alpha,t\right) & := & \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}d\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In many EB applications (e.g.
 example below), 
\begin_inset Formula $M\left(\alpha,t\right)$
\end_inset

 has a closed form since 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 are conjugate and the marginal is a ratio of normalizing constants.
 Specifically,
\begin_inset Formula 
\begin{eqnarray*}
p\left(x,\theta,\alpha\right) & = & p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\\
p_{t}\left(x,\theta,\alpha\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta d\alpha}\\
p_{t}\left(x\vert\alpha\right) & = & \frac{\int p_{t}\left(x,\theta,\alpha\right)d\theta}{p_{t}\left(\alpha\right)}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta d\alpha}\frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}{\left(\int p\left(x\vert\theta\right)dx\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}\\
 & = & \int p\left(x\vert\theta\right)\frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'}d\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus in the marginalized formula it suffices to increase by 
\begin_inset Formula $t$
\end_inset

 the natural parameter in front of the natural sufficient statistic 
\begin_inset Formula $\theta$
\end_inset

 in the distribution 
\begin_inset Formula $p_{t}\left(\theta\vert\alpha\right)$
\end_inset

 for all calculations.
 That is, we can use all the original posterior formulas but replacing 
\begin_inset Formula $p\left(\theta\vert\alpha\right)$
\end_inset

 with 
\begin_inset Formula $p_{t}\left(\theta\vert\alpha\right)$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(\theta\vert\alpha\right) & = & \frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
One could also take the MAP estimates, observing that
\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(\alpha\vert x\right) & = & \frac{p_{t}\left(x\vert\alpha\right)p\left(\alpha\right)}{p_{t}\left(x\right)}\\
M_{MAP} & = & \log p_{t}\left(x\vert\alpha\right)+\log p\left(\alpha\right)-\log p_{t}\left(x\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
In any case, since for any 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 is chosen so that
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial M}{\partial\alpha}\right|_{t} & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
we can differentiate using the chain rule, giving
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\frac{d\alpha}{dt}+\frac{\partial^{2}M}{\partial\alpha\partial t} & = & 0\Rightarrow\\
\frac{d\alpha}{dt} & = & -\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that since 
\begin_inset Formula $M$
\end_inset

 is maximized, its Hessian will be negative definite.
 
\end_layout

\begin_layout Subsection
\begin_inset Formula $d\mbe_{p_{t}}\left[\theta\vert x,\alpha\right]/d\alpha$
\end_inset


\end_layout

\begin_layout Standard
This derivative can sometimes be calculated exactly, or it can be evaluated
 with MCMC draws by differentiating under the integral.
\end_layout

\begin_layout Subsubsection
With analytic expectation
\end_layout

\begin_layout Standard
In certain cases (e.g.
 conjugate exponential families), 
\begin_inset Formula $\mbe_{p_{t}}\left[\theta\vert x,\alpha\right]=m\left(\alpha,t\right)$
\end_inset

 is a known function of 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray*}
\Sigma_{\theta} & = & \frac{dm}{dt^{T}}\\
 & = & \frac{\partial m}{\partial\alpha^{T}}\frac{d\alpha}{dt^{T}}+\frac{\partial m}{\partial t}\\
 & = & -\frac{\partial m}{\partial\alpha^{T}}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}+\frac{\partial m}{\partial t}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\partial m/\partial t$
\end_inset

 is the covariance for fixed 
\begin_inset Formula $\alpha$
\end_inset

.
 Strangely, the covariance correction is not guaranteed to be symmetric
 or positive definite.
 Does this represent an error? (See exponential family examples below.)
\end_layout

\begin_layout Subsubsection
From MCMC draws
\end_layout

\begin_layout Standard
This section is essentially an MCMC estimate of the derivative by differentiatin
g under the integral.
 An cleaner derivation is certainly possible.
\end_layout

\begin_layout Standard
Suppose we have 
\begin_inset Formula $D$
\end_inset

 MCMC draws from 
\begin_inset Formula $p\left(\theta\vert x,\hat{\alpha}\right)$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\theta_{1},...,\theta_{N} & \sim & p\left(\theta\vert x,\hat{\alpha}\right)\\
\mbe_{p}\left[\theta\vert x,\hat{\alpha}\right] & \approx & \frac{1}{D}\sum_{d=1}^{D}\theta_{d}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As a function of 
\begin_inset Formula $t$
\end_inset

, we can calculate expectations with respect to 
\begin_inset Formula $p_{t}\left(\theta\vert x,\hat{\alpha}_{t}\right)$
\end_inset

 using importance sampling:
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{p_{t}}\left[\theta\vert x,\hat{\alpha}_{t}\right] & \approx & \sum_{d=1}^{D}\bar{w}_{d,t}\theta_{d}\\
w_{d,t} & = & \exp\left(\ell\left(\theta_{d},\hat{\alpha}_{t},t\right)-c\left(t\right)-\ell\left(\theta_{d},\hat{\alpha},0\right)+c\left(0\right)\right)\\
\bar{w}_{d,t} & = & \frac{w_{d,t}}{\sum_{d'}w_{d',t}}\\
\hat{\Sigma}_{\theta} & := & \frac{d}{dt}\mbe_{p_{t}}\left[\theta\vert x,\hat{\alpha}_{t}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This can be differentiated:
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt^{T}}\sum_{d=1}^{D}w_{d,t}\theta_{d} & = & \sum_{d=1}^{D}\theta_{d}\frac{d\bar{w}_{d,t}}{dt^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
By the product rule,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\bar{w}_{d,t}}{dt} & = & \frac{1}{\sum_{d'}w_{d',t}}\frac{dw_{d,t}}{dt}-\frac{w_{d,t}}{\left(\sum_{d'}w_{d',t}\right)^{2}}\sum_{d''}\frac{dw_{d'',t}}{dt}\\
 & = & \frac{1}{\sum_{d'}w_{d',t}}\left(\frac{dw_{d,t}}{dt}-\frac{w_{d,t}}{\sum_{d'}w_{d',t}}\sum_{d''}\frac{dw_{d'',t}}{dt}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\sum_{d}\frac{d\bar{w}_{d,t}}{dt} & = & \frac{1}{\sum_{d'}w_{d',t}}\left(\sum_{d}\frac{dw_{d,t}}{dt}-\frac{\sum_{d}w_{d,t}}{\sum_{d'}w_{d',t}}\sum_{d''}\frac{dw_{d'',t}}{dt}\right)\\
 & = & \frac{1}{\sum_{d'}w_{d',t}}\left(\sum_{d}\frac{dw_{d,t}}{dt}-\sum_{d''}\frac{dw_{d'',t}}{dt}\right)\\
 & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Consequently the estimate is shift invariant in 
\begin_inset Formula $\theta$
\end_inset

, as would be hoped.
 This is just a sanity check, since automatically:
\begin_inset Formula 
\begin{eqnarray*}
\sum_{d}\bar{w}_{d,t} & = & 1\Rightarrow\\
\sum_{d}\frac{d\bar{w}_{d,t}}{dt} & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Above,
\begin_inset Formula 
\begin{eqnarray*}
\frac{dw_{d,t}}{dt} & = & \frac{d\alpha^{T}}{dt}\frac{\partial w_{d,t}}{\partial\alpha}+\frac{\partial w_{d,t}}{\partial t}\\
\frac{\partial w_{d,t}}{\partial\alpha} & = & w_{d,t}\frac{\partial\ell\left(\theta_{d},\alpha,t\right)}{\partial\alpha}\\
\frac{\partial w_{d,t}}{\partial t} & = & w_{d,t}\left(\frac{\partial\ell\left(\theta_{d},\alpha,t\right)}{\partial t}-\frac{\partial c\left(t\right)}{\partial t}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From above, we can see that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\ell\left(\theta_{d},\alpha,t\right)}{\partial t}-\frac{\partial c\left(t\right)}{\partial t} & = & \frac{\theta_{d}\cdot p\left(x\vert\theta_{d},\alpha\right)p\left(\theta_{d}\right)p\left(\alpha\right)e^{t\theta_{d}}}{p\left(x\vert\theta_{d},\alpha\right)p\left(\theta_{d}\right)p\left(\alpha\right)e^{t\theta_{d}}}-\frac{\int\exp\left(\ell\left(\alpha,t\right)\right)\theta d\theta d\alpha}{\int\exp\left(\ell\left(\alpha,t\right)\right)d\theta d\alpha}\\
 & = & \theta_{d}-\mbe_{p_{t}}\left[\theta\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Denoting
\begin_inset Formula 
\begin{eqnarray*}
\ell_{\alpha,d} & := & \frac{\partial\ell\left(\theta_{d},\alpha,0\right)}{\partial\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And substituting again
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial c\left(t\right)}{\partial t}\right|_{t=0}=\mbe_{p}\left[\theta\right] & \approx & \frac{1}{D}\sum_{d=1}^{D}\theta_{n}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
then plugging in
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\bar{w}_{d,t}}{dt} & = & \frac{1}{\sum_{d'}w_{d',t}}\left(w_{d,t}\left(\frac{d\alpha^{T}}{dt}\frac{\partial\ell\left(\theta_{d},\alpha,t\right)}{\partial\alpha^{T}}+\theta_{d}-\mbe_{p_{t}}\left[\theta\right]\right)-\frac{w_{d,t}}{\sum_{d'}w_{d',t}}\sum_{d''}\frac{dw_{d'',t}}{dt}\right)\\
\left.\frac{d\bar{w}_{d,t}}{dt}\right|_{t=0} & = & \frac{1}{D}\left(\frac{d\alpha^{T}}{dt}\ell_{\alpha,d}+\theta_{d}-\mbe_{p_{t}}\left[\theta\right]-\frac{1}{D}\sum_{d'}\left(\frac{d\alpha^{T}}{dt}\ell_{\alpha,d'}+\theta_{d'}-\mbe_{p_{t}}\left[\theta\right]\right)\right)\\
 & = & \frac{1}{D}\left(\ell_{\alpha,d}^{T}\frac{d\alpha}{dt}+\theta_{d}-\frac{1}{D}\sum_{d'}\left(\frac{d\alpha^{T}}{dt}\ell_{\alpha,d'}+\theta_{d'}\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Of course, in general
\begin_inset Formula 
\begin{eqnarray*}
\sum x\left(y-\bar{y}\right) & = & \sum\left(x-\bar{x}\right)\left(y-\bar{y}\right)=\sum\left(x-\bar{x}\right)y
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So we can center 
\begin_inset Formula $\theta$
\end_inset

 instead of 
\begin_inset Formula $d\bar{w}/dt$
\end_inset

 for simplicity to get 
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \left.\frac{d}{dt^{T}}\sum_{d=1}^{D}w_{d,t}\theta_{d}\right|_{t=0}\\
 & = & \frac{1}{D}\sum_{d=1}^{D}\left(\theta_{d}-\bar{\theta}\right)\left(\ell_{\alpha,d}^{T}\frac{d\alpha}{dt^{T}}+\theta_{d}^{T}\right)\\
 & = & \frac{1}{D}\sum_{d=1}^{D}\left(\theta_{d}-\bar{\theta}\right)\ell_{\alpha,d}^{T}\left.\frac{d\alpha}{dt^{T}}\right|_{\alpha=\hat{\alpha}}+\widehat{\textrm{Cov}\left(\theta\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\widehat{\textrm{Cov}\left(\theta\right)}$
\end_inset

 is the usual MCMC sampling covariance.
 
\end_layout

\begin_layout Standard
Putting this all together, we get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \widehat{\textrm{Cov}\left(\theta\right)}-\frac{1}{D}\sum_{d=1}^{D}\left(\theta_{d}-\bar{\theta}\right)\ell_{\hat{\alpha},d}^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Exponential families
\end_layout

\begin_layout Standard
If we have a conjugate exponential family:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(x_{i}\vert\theta_{i}\right) & = & \theta_{i}^{T}x_{i}-A_{x}\left(\theta_{i}\right)\\
\log p\left(\theta_{i}\vert\alpha\right) & = & \alpha_{\theta}^{T}\theta_{i}-\alpha_{x}A_{x}\left(\theta_{i}\right)-A\left(\alpha_{\theta},\alpha_{x}\right)\\
p_{t}\left(\theta_{i}\vert\alpha\right) & = & \frac{p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}-t_{A,i}A_{x}\left(\theta_{i}\right)}}{\int p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}+t_{A,i}A_{x}\left(\theta_{i}\right)}d\theta_{i}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(x\vert\alpha\right) & = & \int\prod_{i}p\left(x_{i}\vert\theta_{i}\right)p_{t}\left(\theta_{i}\vert\alpha\right)p\left(\alpha\right)d\theta\\
 & = & \frac{\int\prod_{i}p\left(x_{i}\vert\theta_{i}\right)p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}-t_{A,i}A_{x}\left(\theta_{i}\right)}p\left(\alpha\right)d\theta}{\int\prod_{i}p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}+t_{A,i}A_{x}\left(\theta_{i}\right)}d\theta_{i}}\\
 & = & \frac{\int\exp\left(\sum_{i}\left(x_{i}+\alpha_{\theta}+t_{i}\right)^{T}\theta_{i}-\sum_{i}\left(1+\alpha_{x}+t_{A,i}\right)A_{x}\left(\theta_{i}\right)\right)d\theta}{\int\exp\left(\sum_{i}\left(\alpha_{\theta}+t_{i}\right)^{T}\theta-\sum_{i}\left(\alpha_{x}+t_{A,i}\right)A_{x}\left(\theta_{i}\right)\right)d\theta}\\
 & = & \exp\left(\sum_{i}\left(A\left(x_{i}+\alpha_{\theta}+t_{i},1+\alpha_{x}+t_{A,i}\right)-A\left(\alpha_{\theta}+t_{i},\alpha_{x}+t_{A,i}\right)\right)\right)\\
 & =: & \exp\left(\sum_{i}\left(A\left(x_{i}+\alpha+t_{i}\right)-A\left(\alpha+t_{i}\right)\right)\right)\textrm{ (slight abuse of notation)}
\end{eqnarray*}

\end_inset

In the case of the MLE for an exponential family,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
M & = & \sum_{i}\left(A\left(x_{i}+\alpha+t_{i}\right)-A\left(\alpha+t_{i}\right)\right)\\
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}} & = & \sum_{i}\left(\textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\right)-\textrm{Var}_{p\left(\theta_{i}\vert\alpha\right)}\left(\theta_{i}\right)\right)\\
\frac{\partial^{2}M}{\partial\alpha\partial t_{i}^{t}} & = & \textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\right)-\textrm{Var}_{p\left(\theta_{i}\vert\alpha\right)}\left(\theta_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The expectation is given by
\begin_inset Formula 
\begin{eqnarray*}
\log p_{t}\left(\theta_{i}\vert\alpha,x\right) & = & \left(\alpha_{\theta}+t_{i}\right)^{T}\theta_{i}-\left(\alpha_{x}+t_{A,i}\right)A_{x}\left(\theta_{i}\right)-A\left(\alpha_{\theta}+t_{i},\alpha_{x}+t_{A,i}\right)\Rightarrow\\
\frac{\partial\mbe_{p_{t}}\left[\theta_{i}\vert\alpha,x\right]}{\partial\alpha} & = & \textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\vert\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that since 
\begin_inset Formula $\frac{\partial^{2}M}{\partial\alpha\partial t_{i}^{t}}\ne\frac{\partial\mbe_{p_{t}}\left[\theta_{i}\vert\alpha,x\right]}{\partial\alpha}$
\end_inset

, the correction is not symmetric, nor is it guaranteed to be positive definite.
 Simulations also suggest that a better correction uses 
\begin_inset Formula $\frac{\partial^{2}M}{\partial\alpha\partial t_{i}^{t}}=\textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\right)$
\end_inset

, which would also result in a symmetric, positive definite corrected matrix.
\end_layout

\begin_layout Standard

\series bold
\color red
Alp: can you find an error that would support this?
\end_layout

\begin_layout Section
Gamma Poisson Example
\end_layout

\begin_layout Standard
Let's take a classic EB problem, a simple Gamma-Poisson model.
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\vert\lambda_{i} & \sim & \textrm{Poisson}\left(\lambda_{i}\right)\\
\lambda_{i} & \sim & \textrm{Gamma}\left(\gamma,\beta\right)\\
\mbe\left[\lambda_{i}\right] & = & \frac{\gamma}{\beta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Additionally, we may put gamma priors on 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 (at the risk of having too many gammas around):
\begin_inset Formula 
\begin{eqnarray*}
\gamma & \sim & \textrm{Gamma}\left(a_{\gamma},b_{\gamma}\right)\\
\beta & \sim & \textrm{Gamma}\left(a_{\beta},b_{\beta}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Variance of 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard
This can be marginalized exactly as a function of 
\begin_inset Formula $t$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}-\log y_{i}!\\
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\beta-\log\Gamma\left(\gamma\right)-\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log p\left(y,\lambda\vert\gamma,\beta\right) & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\log y_{i}!\\
 &  & +\sum_{i}\left[-\left(\beta+1\right)\lambda_{i}+\left(\gamma+y_{i}-1\right)\log\lambda_{i}+\left(\gamma+y_{i}\right)\log\left(\beta+1\right)-\log\Gamma\left(\gamma+y_{i}\right)\right]\\
 &  & -\sum_{i}\left[\left(\gamma+y_{i}\right)\log\left(\beta+1\right)-\log\Gamma\left(\gamma+y_{i}\right)\right]\Rightarrow\\
\log p\left(y\vert\gamma,\beta\right) & = & N\left(\gamma\log\beta-\log\Gamma\left(\gamma\right)\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!\\
\log p_{t}\left(y\vert\gamma,\beta\right) & = & \sum_{i}\gamma\log\left(\beta-t_{i}\right)-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1-t_{i}\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
Marginal likelihood
\end_layout

\begin_layout Standard
We choose 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 to maximize the marginal likelihood, so that
\begin_inset Formula 
\begin{eqnarray*}
M & = & \log p_{t}\left(y\vert\gamma,\beta\right)\\
 & = & \sum_{i}\gamma\log\left(\beta-t_{i}\right)-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1-t_{i}\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The derivatives are then
\begin_inset Formula 
\begin{eqnarray*}
0=\frac{\partial}{\partial\gamma}\log p_{t} & = & \sum_{i}\log\left(\beta-t_{i}\right)-N\psi\left(\gamma\right)-\sum_{i}\log\left(\beta+1-t_{i}\right)+\sum_{i}\psi\left(\gamma+y_{i}\right)\\
0=\frac{\partial}{\partial\beta}\log p_{t} & = & \sum_{i}\left(\frac{\gamma}{\beta-t_{i}}-\frac{\gamma+y_{i}}{1+\beta-t_{i}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}} & = & N\left(\begin{array}{cc}
\frac{1}{N}\sum_{i}\left(\psi'\left(\gamma+y_{i}\right)-\psi'\left(\gamma\right)\right) & \frac{1}{N}\sum_{i}\left(\frac{1}{\beta-t_{i}}-\frac{1}{1+\beta-t_{i}}\right)\\
\frac{1}{N}\sum_{i}\left(\frac{1}{\beta-t_{i}}-\frac{1}{1+\beta-t_{i}}\right) & \frac{1}{N}\sum_{i}\left(\frac{\gamma+y_{i}}{\left(1+\beta-t_{i}\right)^{2}}-\frac{\gamma}{\left(\beta-t_{i}\right)^{2}}\right)
\end{array}\right)\\
\frac{\partial^{2}M}{\partial\alpha\partial t_{i}} & = & \left(\begin{array}{c}
\frac{1}{1+\beta-t_{i}}-\frac{1}{\beta-t_{i}}\\
\frac{\gamma}{\left(\beta-t_{i}\right)^{2}}-\frac{\gamma+y_{i}}{\left(1+\beta-t_{i}\right)^{2}}
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that here 
\begin_inset Formula $N$
\end_inset

 refers to observations within a sample, not the number of MCMC draws, which
 is denoted by 
\begin_inset Formula $D$
\end_inset

.
 Also, the leading 
\begin_inset Formula $N$
\end_inset

 in the Hessian cancels the leading 
\begin_inset Formula $N$
\end_inset

 in 
\begin_inset Formula $\ell_{\alpha,d}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
MAP estimates
\end_layout

\begin_layout Standard
Alternatively, we can take the MAP so that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(\gamma,\beta\vert y\right) & = & \frac{p_{t}\left(y\vert\gamma,\beta\right)p\left(\gamma,\beta\right)}{p_{t}\left(y\right)}\\
M_{MAP} & = & \log p_{t}\left(y\vert\gamma,\beta\right)+\log p\left(\gamma,\beta\right)-\log p_{t}\left(y\right)\\
 & = & M+\log p\left(\gamma,\beta\right)-\log p_{t}\left(y\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\log p_{t}\left(y\right)}{\partial\alpha} & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
so that we don't have to worry about the normalizing constant's dependence
 on 
\begin_inset Formula $t$
\end_inset

.
 Since
\begin_inset Formula 
\begin{eqnarray*}
\gamma & \sim & \textrm{Gamma}\left(a_{\gamma},b_{\gamma}\right)\\
\log p\left(\gamma\right) & = & -b_{\gamma}\gamma+\log\left(\gamma\right)\left(a_{\gamma}-1\right)+C\\
\frac{\partial^{2}\log p\left(\gamma\right)}{\partial\gamma^{2}} & = & -\frac{a_{\gamma}-1}{\gamma^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And similarly
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}\log p\left(\beta\right)}{\partial\beta^{2}} & = & -\frac{a_{\beta}-1}{\beta^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
MCMC weights
\end_layout

\begin_layout Standard
Here, we include the priors, to get
\begin_inset Formula 
\begin{eqnarray*}
\ell\left(\theta,\alpha,t\right) & = & \log p_{t}\left(y,\lambda\vert\gamma,\beta\right)+\log p\left(\gamma\right)+\log p\left(\beta\right)\\
 & = & \sum_{i}\gamma\log\left(\beta-t_{i}\right)-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1-t_{i}\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 &  & -b_{\gamma}\gamma+\left(a_{\gamma}-1\right)\log\gamma-b_{\beta}\beta+\left(b_{\beta}-1\right)\log\beta
\end{eqnarray*}

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
\ell_{\hat{\alpha},d} & = & \left(\begin{array}{c}
\frac{\partial}{\partial\gamma}\ell\left(\theta_{d},\hat{\alpha},0\right)\\
\frac{\partial}{\partial\beta}\ell\left(\theta_{d},\hat{\alpha},0\right)
\end{array}\right)\\
\frac{\partial}{\partial\gamma}\ell\left(\theta,\alpha,0\right) & = & N\left(\log\beta-\psi\left(\gamma\right)+\frac{1}{N}\sum_{i}\log\lambda_{i}-b_{\gamma}+\frac{a_{\gamma}-1}{\gamma}\right)\\
\frac{\partial}{\partial\beta}\ell\left(\theta,\alpha,0\right) & = & N\left(\frac{\gamma}{\beta}-\frac{1}{N}\sum\lambda_{i}-b_{\beta}+\frac{a_{\beta}-1}{\beta}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Exact moments
\end_layout

\begin_layout Standard
Instead of using MCMC, in this case we can use exact moments.
 Specifically,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}-\log y_{i}!\\
\log p_{t}\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\left(\beta+t_{i}\right)-\log\Gamma\left(\gamma\right)-\left(\beta-t_{i}\right)\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log p_{t}\left(\lambda_{i}\vert y_{i},\gamma,\beta\right) & = & \left(y_{i}+\gamma\right)\log\left(\beta+1+t_{i}\right)-\log\Gamma\left(y_{i}+\gamma\right)-\left(\beta+1-t_{i}\right)\lambda_{i}+\left(y_{i}+\gamma-1\right)\log\lambda_{i}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\mbe_{p_{t}}\left[\lambda_{i}\vert\alpha,y_{i}\right] & = & \frac{\gamma+y_{i}}{\beta+1-t_{i}}:=m_{i}\left(\alpha,t_{i}\right)\\
\left.\frac{\partial m_{i}}{\partial t_{i}}\right|_{t_{i}=0} & = & \frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}\\
\left.\frac{\partial m_{i}}{\partial\gamma}\right|_{t_{i}=0} & = & \frac{1}{1+\beta}\\
\left.\frac{\partial m_{i}}{\partial\beta}\right|_{t_{i}=0} & = & -\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}_{t}\left(\lambda_{i}\right) & = & -\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\begin{array}{c}
\frac{1}{1+\beta}-\frac{1}{\beta}\\
\frac{\gamma}{\beta^{2}}-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)+\frac{y_{i}+\gamma}{\left(\beta+1\right)^{2}}\\
 & = & -\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\begin{array}{c}
\frac{1}{1+\beta}\\
\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)-\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\begin{array}{c}
-\frac{1}{\beta}\\
\frac{\gamma}{\beta^{2}}
\end{array}\right)+\frac{y_{i}+\gamma}{\left(\beta+1\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Variance of 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
If we try to do the same trick as above,
\begin_inset Formula 
\begin{eqnarray*}
p\left(x,\theta,\alpha\right) & = & p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\\
p_{t}\left(x,\theta,\alpha\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}dxd\theta d\alpha}\\
p_{t}\left(x\vert\alpha\right) & = & \frac{\int p_{t}\left(x,\theta,\alpha\right)d\theta}{p_{t}\left(\alpha\right)}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)dxd\theta}\\
 & = & p\left(x\vert\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
A similar argument will apply to 
\begin_inset Formula $p_{t}\left(x,\theta\vert\alpha\right)$
\end_inset

.
 However, under 
\begin_inset Formula $t\theta$
\end_inset

 perturbations we will still have.
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left[\theta\vert x\right] & = & \mbe\left[\textrm{Var}\left[\theta\vert x,\alpha\right]\vert x\right]+\textrm{Var}\left(\mbe\left[\theta\vert x,\alpha\right]\vert x\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...from which we can infer (maybe) the moments of 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Subsection
Covariances as matrix products
\end_layout

\begin_layout Standard
Here, if the vector draws 
\begin_inset Formula $\lambda_{d}$
\end_inset

 are arranged columnwise:
\begin_inset Formula 
\begin{eqnarray*}
\Lambda & = & \left(\begin{array}{ccc}
\left(\lambda_{1}-\bar{\lambda}\right) & \cdots & \left(\lambda_{D}-\bar{\lambda}\right)\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...and we stack the products 
\begin_inset Formula $\ell_{\hat{\alpha},d}^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}$
\end_inset

 in rows, so that 
\begin_inset Formula 
\begin{eqnarray*}
Q & = & \left(\begin{array}{c}
\ell_{\hat{\alpha},1}^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}\\
\vdots\\
\ell_{\hat{\alpha},D}^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{array}\right)\\
 & = & \left(\begin{array}{c}
\ell_{\hat{\alpha},1}^{T}\\
\vdots\\
\ell_{\hat{\alpha},D}^{T}
\end{array}\right)\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then the covariance correction can be estimated with 
\begin_inset Formula $\frac{1}{D}\Lambda Q$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \widehat{\textrm{Cov}\left(\theta\right)}-\frac{1}{D}\Lambda Q
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Gamma moments
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
x & \sim & \textrm{Gamma}\left(\alpha,\beta\right)\\
p\left(x\right) & = & \frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}x^{\alpha-1}e^{-\beta x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[x^{k}\right] & = & \int\frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}x^{\alpha-1}e^{-\beta x}x^{k}dx\\
 & = & \frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}\frac{\Gamma\left(k+\alpha\right)}{\beta^{k+\alpha}}\int\frac{\beta^{k+\alpha}}{\Gamma\left(k+\alpha\right)}x^{k+\alpha-1}e^{-\beta x}dx\\
 & = & \frac{\Gamma\left(k+\alpha\right)}{\Gamma\left(\alpha\right)}\beta^{-k}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From the internet, the variance of the sample variance is given by
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(s^{2}\right) & = & \frac{\left(N-1\right)^{2}}{N^{3}}\mu_{4}-\frac{\left(N-1\right)\left(N-3\right)\mu_{2}^{2}}{N^{3}}\\
\mu_{4} & = & \mbe\left[\left(x-\mu\right)^{4}\right]\\
 & = & \mbe\left[x^{4}-4x^{3}\mu+6x^{2}\mu^{2}-4x\mu^{3}+\mu^{4}\right]\\
 & = & m_{4}-4m_{3}\mu+6m_{2}\mu^{2}-4\mu^{4}+\mu^{4}\\
 & = & m_{4}-4m_{3}\mu+6m_{2}\mu^{2}-3\mu^{4}\\
\mu_{2} & = & m_{2}-\mu^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then by the delta method
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(\sqrt{x}\right) & \approx & \frac{\textrm{Var}\left(x\right)}{4\mbe\left[x\right]}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Normal-Normal Example
\end_layout

\begin_layout Standard
Let's take an even more classic example:
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\theta_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,\tau^{-1}\right)\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\tau,\tau_{\alpha} &  & \textrm{are known}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(x_{i}\vert\theta_{i}\right) & = & -\frac{1}{2}\left(x_{i}-\theta_{i}\right)^{2}+C\\
 & = & -\frac{1}{2}x_{i}^{2}+x_{i}\theta_{i}-\frac{1}{2}\theta_{i}^{2}\\
\log p\left(\theta_{i}\vert\alpha\right) & = & -\frac{1}{2}\left(\tau\left(\theta_{i}-\alpha\right)^{2}-\log\tau\right)+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha-\frac{1}{2}\tau\alpha^{2}-\frac{1}{2}\log\tau+C\\
\log p\left(\alpha\right) & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
True posterior: integrating out 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
Here, we know that marginally over 
\begin_inset Formula $\alpha$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i} & \sim & \mathcal{N}\left(0,\tau^{-1}+\tau_{\alpha}^{-1}\right)\\
\tau_{m} & := & \left(\tau^{-1}+\tau_{\alpha}^{-1}\right)^{-1}\\
 & = & \frac{\tau\tau_{\alpha}}{\tau+\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\tau_{m}^{-1} & = & \tau^{-1}+\tau_{\alpha}^{-1}>\tau^{-1}\Rightarrow\\
\tau & > & \tau_{m}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus the posterior is given by
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert x_{i}\right] & = & \frac{1}{\tau_{m}+1}\left(0\cdot\tau_{m}+x_{i}\right)\\
 & = & \frac{x_{i}}{\tau_{m}+1}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert x_{i} & \sim & \mathcal{N}\left(\frac{x_{i}}{\tau_{m}+1},\left(\tau_{m}+1\right)^{-1}\right)\\
\frac{1}{\tau_{m}+1} & = & \frac{\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau+\tau\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is the variance we hope to recover with linear response.
\end_layout

\begin_layout Subsection
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset


\end_layout

\begin_layout Standard
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,1+\tau^{-1}\right)\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\mbe\left[\alpha\vert x_{i}\right] & = & \frac{1}{\tau_{\alpha}+\left(1+\tau^{-1}\right)^{-1}}\left(\left(1+\tau^{-1}\right)^{-1}x_{i}\right)\\
 & = & \frac{1}{\left(1+\tau^{-1}\right)\tau_{\alpha}+1}x_{i}\\
 & = & \frac{\tau}{\left(\tau+1\right)\tau_{\alpha}+\tau}x_{i}\\
 & = & \frac{\tau}{\tau\tau_{\alpha}+\tau_{\alpha}+\tau}x_{i}\\
\frac{\tau}{\tau\tau_{\alpha}+\tau_{\alpha}+\tau} & = & \frac{\tau}{\left(\frac{\tau\tau_{\alpha}}{\tau+\tau_{\alpha}}+1\right)}\frac{1}{\left(\tau+\tau_{\alpha}\right)}\frac{\tau_{\alpha}}{\tau_{\alpha}}\\
 & = & \frac{\tau_{m}}{\tau_{\alpha}\left(\tau_{m}+1\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that this is maximized at 
\begin_inset Formula $\hat{\alpha}=\frac{\tau}{\tau\tau_{\alpha}+\tau_{\alpha}+\tau}x_{i}$
\end_inset

.
 Thus
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\hat{\alpha} & \sim & \mathcal{N}\left(\frac{\tau_{m}}{\tau_{\alpha}\left(\tau_{m}+1\right)}x_{i},\tau^{-1}\right)\\
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\textrm{Var}\left(\theta_{i}\vert\hat{\alpha},x_{i}\right) & = & \left(1+\tau\right)^{-1}\\
\mbe\left[\theta_{i}\vert\hat{\alpha},x_{i}\right] & = & \frac{1}{1+\tau}\left(\frac{\tau_{m}\tau}{\tau_{\alpha}\left(\tau_{m}+1\right)}+1\right)x_{i}\\
\frac{\tau_{m}\tau}{\tau_{\alpha}\left(\tau_{m}+1\right)\left(1+\tau\right)} & = & \frac{\tau_{m}\tau}{\left(\tau_{m}+1\right)\left(\tau_{\alpha}+\tau_{\alpha}\tau\right)}\\
 & = & \frac{\tau_{m}\tau}{\left(\tau_{m}+1\right)\left(\frac{\tau_{\alpha}}{\tau+\tau_{\alpha}}+\tau_{m}\right)\left(\tau+\tau_{\alpha}\right)}\\
 & = & \frac{\tau_{m}^{2}}{\left(\tau_{m}+1\right)\left(\frac{\tau_{\alpha}}{\tau+\tau_{\alpha}}+\tau_{m}\right)\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Old, wrong:
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\hat{\alpha} & \sim & \mathcal{N}\left(\frac{x_{i}}{1+\tau_{\alpha}},\tau^{-1}\right)\\
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\theta_{i}\vert\hat{\alpha},x_{i} & \sim & \mathcal{N}\left(\frac{1}{\tau+1}\left(\tau\hat{\alpha}+x_{i}\right),\left(1+\tau\right)^{-1}\right)\\
 & = & \mathcal{N}\left(\frac{1+\tau_{\alpha}+\tau}{1+\tau_{\alpha}\tau+\tau+\tau_{\alpha}}x_{i},\left(1+\tau\right)^{-1}\right)\\
\frac{1}{\tau+1}\left(\tau\frac{x_{i}}{1+\tau_{\alpha}}+x_{i}\right) & = & \frac{1+\tau_{\alpha}+\tau}{\left(\tau+1\right)\left(1+\tau_{\alpha}\right)}x_{i}\\
 & = & \frac{1+\tau_{\alpha}+\tau}{1+\tau_{\alpha}\tau+\tau+\tau_{\alpha}}x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Here, in contrast to the correct model with 
\begin_inset Formula $\alpha$
\end_inset

 marginalized out, 
\begin_inset Formula $\theta_{i}\vert\hat{\alpha},x_{i}$
\end_inset

 is shrunk less towards 
\begin_inset Formula $0$
\end_inset

 and its variance is too small since 
\begin_inset Formula $\tau>\tau_{m}$
\end_inset

.
\end_layout

\begin_layout Subsection
Tilted model
\end_layout

\begin_layout Standard
Suppose we are interested in the variance of 
\begin_inset Formula $\theta_{i}$
\end_inset

.
 Then we perturb with 
\begin_inset Formula $t_{i}\theta_{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p_{t}\left(\theta_{i}\vert\alpha\right) & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha+t_{i}\theta_{i}+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\left(\alpha+\tau^{-1}t_{i}\right)\theta_{i}+C\\
 & = & -\frac{1}{2}\tau\left(\theta_{i}-\left(\alpha+\tau^{-1}t_{i}\right)\right)^{2}+C
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau^{-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Consequently,
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\alpha,t_{i},x_{i} & \sim & \mathcal{N}\left(\frac{1}{1+\tau}\left(\tau\left(\alpha+\tau^{-1}t_{i}\right)+x_{i}\right),\left(1+\tau\right)^{-1}\right)\\
\frac{1}{1+\tau}\left(\tau\left(\alpha+\tau^{-1}t_{i}\right)+x_{i}\right) & = & \frac{\tau\alpha+t_{i}+x_{i}}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert\alpha,t_{i},x_{i}\right] & = & \frac{\tau\alpha+t_{i}+x_{i}}{1+\tau}\\
m & := & \mbe\left[\theta_{i}\vert\alpha,t_{i},x_{i}\right]\\
\frac{\partial m}{\partial t_{i}} & = & \frac{1}{1+\tau}\\
\frac{\partial m}{\partial\alpha} & = & \frac{\tau}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Next, we see that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},1+\tau^{-1}\right)\Rightarrow\\
x_{i}-\tau^{-1}t_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha,1+\tau^{-1}\right)\\
\alpha\vert x_{i},t_{i} & \sim & \mathcal{N}\left(\frac{1}{1+\tau_{m}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As a function of 
\begin_inset Formula $\alpha$
\end_inset

, this is maximized at
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}+\tau^{-1}t_{i} & = & x_{i}\\
\hat{\alpha} & = & x_{i}-\tau^{-1}t_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\hat{\alpha}}{dt} & = & -\tau^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Putting this together, we get that
\begin_inset Formula 
\begin{eqnarray*}
\frac{dm}{dt} & = & \left(1+\tau\right)^{-1}+\frac{\tau}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Linear response correction
\end_layout

\begin_layout Standard
OLD MAYBE NOT RIGHT
\end_layout

\begin_layout Standard
Of course
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{t}\left[\theta_{i}\vert\alpha\right] & = & \alpha+\tau^{-1}t_{i}\\
\frac{\partial}{\partial t_{i}}\mbe_{t}\left[\theta_{i}\vert\alpha\right] & = & \tau^{-1}\\
\frac{\partial}{\partial\alpha}\mbe_{t}\left[\theta_{i}\vert\alpha\right] & = & 1
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For a given 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $t_{i}$
\end_inset

, we then have
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},1+\tau^{-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
M_{MLE,t,i}\left(\alpha,t\right) & = & -\frac{1}{2}\left(1+\tau^{-1}\right)^{-1}\left(x_{i}-\left(\alpha+\tau^{-1}t_{i}\right)\right)^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For a given 
\begin_inset Formula $t_{i}$
\end_inset

, this is maximized at
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{EB} & = & x_{i}-\tau^{-1}t_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{t}\left[\theta_{i}\vert\hat{\alpha}_{EB},x_{i}\right] & = & \frac{1}{1+\tau}\left(\tau\hat{\alpha}_{EB}+x_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We have
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial M_{MLE}}{\partial\alpha}\right|_{t_{i}} & = & \left(1+\tau^{-1}\right)^{-1}\left(x_{i}-\alpha-\tau^{-1}t_{i}\right)\\
\left.\frac{\partial M_{MLE}}{\partial\alpha}\right|_{t_{i}=0} & = & \left(1+\tau^{-1}\right)^{-1}\left(x_{i}-\alpha\right)\\
\left.\frac{\partial^{2}M_{MLE}}{\partial\alpha^{2}}\right|_{t_{i}=0} & = & -\left(1+\tau^{-1}\right)^{-1}\\
\left.\frac{\partial^{2}M_{MLE}}{\partial\alpha\partial t}\right|_{t_{i}=0} & = & -\tau^{-1}\left(1+\tau^{-1}\right)^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}\log p\left(\alpha\right)}{\partial\alpha^{2}} & = & -\tau_{\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & -\left(\frac{\partial^{2}M}{\partial\alpha^{2}}\right)^{-1}\frac{\partial^{2}M_{MLE}}{\partial\alpha\partial t}\\
 & = & \tau^{-1}\left(\left(1+\tau^{-1}\right)^{-1}-\tau_{\alpha}\right)^{-1}\left(1+\tau^{-1}\right)^{-1}\\
\left(\frac{d\alpha}{dt}\right)^{-1} & = & \tau\left(\frac{1}{1+\tau^{-1}}-\tau_{\alpha}\right)\left(1+\tau^{-1}\right)\\
 & = & \left(\frac{\tau}{\tau+1}-\tau_{\alpha}\right)\left(\tau+1\right)\\
 & = & \left(\tau+\tau_{\alpha}\left(\tau+1\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
m\left(t,\alpha_{t}\right) & = & \mbe_{t_{i}}\left[\theta_{i}\vert\alpha_{t}\right]\\
\frac{dm}{dt} & = & \frac{\partial m}{\partial\alpha}\frac{d\alpha}{dt}+\frac{\partial m}{\partial t}\\
 & = & \left(\tau+\tau_{\alpha}\left(\tau+1\right)\right)^{-1}+\tau^{-1}\\
 & = & \frac{1}{\tau+\tau\tau_{\alpha}+\tau_{\alpha}}+\frac{1}{\tau}\\
 & = & \frac{\tau+\tau_{\alpha}\tau+\tau_{\alpha}+\tau}{\tau\left(\tau+\tau_{\alpha}\tau+\tau_{\alpha}\right)}\\
 & = & \frac{\tau+\tau_{\alpha}\tau+\tau_{\alpha}+\tau}{\tau\left(\tau+\tau_{\alpha}\tau+\tau_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau\tau_{\alpha}+\tau} & = & \textrm{target}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Exact moments from MCMC
\end_layout

\begin_layout Standard
Note that we can use the law of total variance to avoid sampling variance
 for 
\begin_inset Formula $\lambda$
\end_inset

 standard deviations.
 Suppose we know exactly (e.g.
 because of conjugacy) 
\begin_inset Formula $\mbe\left[\theta\vert\alpha\right]$
\end_inset

 and 
\begin_inset Formula $\textrm{Var}\left(\theta\vert\alpha\right)$
\end_inset

.
 Then under 
\begin_inset Formula $\alpha$
\end_inset

 sampling
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(\theta\vert X\right) & = & \mbe\left[\textrm{Var}\left(\theta\vert\alpha,X\right)\vert X\right]+\textrm{Var}\left(\mbe\left[\theta\vert\alpha,X\right]\vert X\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...where the outer expectations can be done by Monte Carlo.
\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Part
Old writing, ignore past this point
\end_layout

\begin_layout Standard
Sanity check: the negative binomial distribution (using the Wikipedia, not
 the R parameterization) is
\begin_inset Formula 
\begin{eqnarray*}
\beta & = & \frac{1-\pi}{\pi}\\
\pi\beta+\pi & = & 1\\
\pi & = & \frac{1}{1+\beta}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(y_{i}\vert\gamma,\pi\right) & = & \frac{\Gamma\left(y_{i}+\gamma\right)}{\Gamma\left(\gamma\right)y!}\pi^{\gamma}\left(1-\pi\right)^{y_{i}}\\
\log p_{t}\left(y_{i}\vert\gamma,\pi\right) & = & \log\Gamma\left(y_{i}+\gamma\right)-\log\Gamma\left(\gamma\right)-\log y!+\gamma\log\left(1-\pi\right)+y_{i}\log\pi\\
 & = & \log\Gamma\left(y_{i}+\gamma\right)-\log\Gamma\left(\gamma\right)-\log y!+\gamma\log\left(\frac{\beta}{1+\beta}\right)+y_{i}\log\left(\frac{1}{1+\beta}\right)\\
 & = & \log\Gamma\left(y_{i}+\gamma\right)-\log\Gamma\left(\gamma\right)-\log y!-\gamma\log\left(1+\beta\right)+\gamma\log\left(\beta\right)-y_{i}\log\left(1+\beta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Re-fitting the moments
\end_layout

\begin_layout Section
Normal-Normal Example
\end_layout

\begin_layout Standard
Let's take an even more classic example:
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\theta_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,\tau^{-1}\right)\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\tau,\tau_{\alpha} &  & \textrm{are known}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(x_{i}\vert\theta_{i}\right) & = & -\frac{1}{2}\left(x_{i}-\theta_{i}\right)^{2}+C\\
 & = & -\frac{1}{2}x_{i}^{2}+x_{i}\theta_{i}-\frac{1}{2}\theta_{i}^{2}\\
\log p\left(\theta_{i}\vert\alpha\right) & = & -\frac{1}{2}\left(\tau\left(\theta_{i}-\alpha\right)^{2}-\log\tau\right)+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha-\frac{1}{2}\tau\alpha^{2}-\frac{1}{2}\log\tau+C\\
\log p\left(\alpha\right) & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
True posterior: integrating out 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
Here, we know that marginally over 
\begin_inset Formula $\alpha$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i} & \sim & \mathcal{N}\left(0,\tau^{-1}+\tau_{\alpha}^{-1}\right)\\
\tau_{m} & := & \left(\tau^{-1}+\tau_{\alpha}^{-1}\right)^{-1}\\
 & = & \frac{\tau\tau_{\alpha}}{\tau+\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\tau_{m}^{-1} & = & \tau^{-1}+\tau_{\alpha}^{-1}>\tau^{-1}\Rightarrow\\
\tau & > & \tau_{m}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus the posterior is given by
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert x_{i}\right] & = & \frac{1}{\tau_{m}+1}\left(0\cdot\tau_{m}+x_{i}\right)\\
 & = & \frac{x_{i}}{\tau_{m}+1}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert x_{i} & \sim & \mathcal{N}\left(\frac{x_{i}}{\tau_{m}+1},\left(\tau_{m}+1\right)^{-1}\right)\\
\frac{1}{\tau_{m}+1} & = & \frac{\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau+\tau\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is the variance we hope to recover with linear response.
\end_layout

\begin_layout Subsection
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset


\end_layout

\begin_layout Standard
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,1+\tau^{-1}\right)\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\mbe\left[\alpha\vert x_{i}\right] & = & \frac{1}{\tau_{\alpha}+\left(1+\tau^{-1}\right)^{-1}}\left(\left(1+\tau^{-1}\right)^{-1}x_{i}\right)\\
 & = & \frac{1}{\left(1+\tau^{-1}\right)\tau_{\alpha}+1}x_{i}\\
 & = & \frac{\tau}{\left(\tau+1\right)\tau_{\alpha}+\tau}x_{i}\\
 & = & \frac{\tau}{\tau\tau_{\alpha}+\tau_{\alpha}+\tau}x_{i}\\
\frac{\tau}{\tau\tau_{\alpha}+\tau_{\alpha}+\tau} & = & \frac{\tau}{\left(\frac{\tau\tau_{\alpha}}{\tau+\tau_{\alpha}}+1\right)}\frac{1}{\left(\tau+\tau_{\alpha}\right)}\frac{\tau_{\alpha}}{\tau_{\alpha}}\\
 & = & \frac{\tau_{m}}{\tau_{\alpha}\left(\tau_{m}+1\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that this is maximized at 
\begin_inset Formula $\hat{\alpha}=\frac{\tau}{\tau\tau_{\alpha}+\tau_{\alpha}+\tau}x_{i}$
\end_inset

.
 Thus
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\hat{\alpha} & \sim & \mathcal{N}\left(\frac{\tau_{m}}{\tau_{\alpha}\left(\tau_{m}+1\right)}x_{i},\tau^{-1}\right)\\
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\textrm{Var}\left(\theta_{i}\vert\hat{\alpha},x_{i}\right) & = & \left(1+\tau\right)^{-1}\\
\mbe\left[\theta_{i}\vert\hat{\alpha},x_{i}\right] & = & \frac{1}{1+\tau}\left(\frac{\tau_{m}\tau}{\tau_{\alpha}\left(\tau_{m}+1\right)}+1\right)x_{i}\\
\frac{\tau_{m}\tau}{\tau_{\alpha}\left(\tau_{m}+1\right)\left(1+\tau\right)} & = & \frac{\tau_{m}\tau}{\left(\tau_{m}+1\right)\left(\tau_{\alpha}+\tau_{\alpha}\tau\right)}\\
 & = & \frac{\tau_{m}\tau}{\left(\tau_{m}+1\right)\left(\frac{\tau_{\alpha}}{\tau+\tau_{\alpha}}+\tau_{m}\right)\left(\tau+\tau_{\alpha}\right)}\\
 & = & \frac{\tau_{m}^{2}}{\left(\tau_{m}+1\right)\left(\frac{\tau_{\alpha}}{\tau+\tau_{\alpha}}+\tau_{m}\right)\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Old, wrong:
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\hat{\alpha} & \sim & \mathcal{N}\left(\frac{x_{i}}{1+\tau_{\alpha}},\tau^{-1}\right)\\
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\theta_{i}\vert\hat{\alpha},x_{i} & \sim & \mathcal{N}\left(\frac{1}{\tau+1}\left(\tau\hat{\alpha}+x_{i}\right),\left(1+\tau\right)^{-1}\right)\\
 & = & \mathcal{N}\left(\frac{1+\tau_{\alpha}+\tau}{1+\tau_{\alpha}\tau+\tau+\tau_{\alpha}}x_{i},\left(1+\tau\right)^{-1}\right)\\
\frac{1}{\tau+1}\left(\tau\frac{x_{i}}{1+\tau_{\alpha}}+x_{i}\right) & = & \frac{1+\tau_{\alpha}+\tau}{\left(\tau+1\right)\left(1+\tau_{\alpha}\right)}x_{i}\\
 & = & \frac{1+\tau_{\alpha}+\tau}{1+\tau_{\alpha}\tau+\tau+\tau_{\alpha}}x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Here, in contrast to the correct model with 
\begin_inset Formula $\alpha$
\end_inset

 marginalized out, 
\begin_inset Formula $\theta_{i}\vert\hat{\alpha},x_{i}$
\end_inset

 is shrunk less towards 
\begin_inset Formula $0$
\end_inset

 and its variance is too small since 
\begin_inset Formula $\tau>\tau_{m}$
\end_inset

.
\end_layout

\begin_layout Subsection
Tilted model
\end_layout

\begin_layout Standard
Suppose we are interested in the variance of 
\begin_inset Formula $\theta_{i}$
\end_inset

.
 Then we perturb with 
\begin_inset Formula $t_{i}\theta_{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p_{t}\left(\theta_{i}\vert\alpha\right) & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha+t_{i}\theta_{i}+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\left(\alpha+\tau^{-1}t_{i}\right)\theta_{i}+C\\
 & = & -\frac{1}{2}\tau\left(\theta_{i}-\left(\alpha+\tau^{-1}t_{i}\right)\right)^{2}+C
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau^{-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Consequently,
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\alpha,t_{i},x_{i} & \sim & \mathcal{N}\left(\frac{1}{1+\tau}\left(\tau\left(\alpha+\tau^{-1}t_{i}\right)+x_{i}\right),\left(1+\tau\right)^{-1}\right)\\
\frac{1}{1+\tau}\left(\tau\left(\alpha+\tau^{-1}t_{i}\right)+x_{i}\right) & = & \frac{\tau\alpha+t_{i}+x_{i}}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert\alpha,t_{i},x_{i}\right] & = & \frac{\tau\alpha+t_{i}+x_{i}}{1+\tau}\\
m & := & \mbe\left[\theta_{i}\vert\alpha,t_{i},x_{i}\right]\\
\frac{\partial m}{\partial t_{i}} & = & \frac{1}{1+\tau}\\
\frac{\partial m}{\partial\alpha} & = & \frac{\tau}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Next, we see that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},1+\tau^{-1}\right)\Rightarrow\\
x_{i}-\tau^{-1}t_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha,1+\tau^{-1}\right)\\
\alpha\vert x_{i},t_{i} & \sim & \mathcal{N}\left(\frac{1}{1+\tau_{m}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As a function of 
\begin_inset Formula $\alpha$
\end_inset

, this is maximized at
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}+\tau^{-1}t_{i} & = & x_{i}\\
\hat{\alpha} & = & x_{i}-\tau^{-1}t_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\hat{\alpha}}{dt} & = & -\tau^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Putting this together, we get that
\begin_inset Formula 
\begin{eqnarray*}
\frac{dm}{dt} & = & \left(1+\tau\right)^{-1}+\frac{\tau}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Linear response correction
\end_layout

\begin_layout Standard
OLD MAYBE NOT RIGHT
\end_layout

\begin_layout Standard
Of course
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{t}\left[\theta_{i}\vert\alpha\right] & = & \alpha+\tau^{-1}t_{i}\\
\frac{\partial}{\partial t_{i}}\mbe_{t}\left[\theta_{i}\vert\alpha\right] & = & \tau^{-1}\\
\frac{\partial}{\partial\alpha}\mbe_{t}\left[\theta_{i}\vert\alpha\right] & = & 1
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For a given 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $t_{i}$
\end_inset

, we then have
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},1+\tau^{-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
M_{MLE,t,i}\left(\alpha,t\right) & = & -\frac{1}{2}\left(1+\tau^{-1}\right)^{-1}\left(x_{i}-\left(\alpha+\tau^{-1}t_{i}\right)\right)^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For a given 
\begin_inset Formula $t_{i}$
\end_inset

, this is maximized at
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{EB} & = & x_{i}-\tau^{-1}t_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{t}\left[\theta_{i}\vert\hat{\alpha}_{EB},x_{i}\right] & = & \frac{1}{1+\tau}\left(\tau\hat{\alpha}_{EB}+x_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We have
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial M_{MLE}}{\partial\alpha}\right|_{t_{i}} & = & \left(1+\tau^{-1}\right)^{-1}\left(x_{i}-\alpha-\tau^{-1}t_{i}\right)\\
\left.\frac{\partial M_{MLE}}{\partial\alpha}\right|_{t_{i}=0} & = & \left(1+\tau^{-1}\right)^{-1}\left(x_{i}-\alpha\right)\\
\left.\frac{\partial^{2}M_{MLE}}{\partial\alpha^{2}}\right|_{t_{i}=0} & = & -\left(1+\tau^{-1}\right)^{-1}\\
\left.\frac{\partial^{2}M_{MLE}}{\partial\alpha\partial t}\right|_{t_{i}=0} & = & -\tau^{-1}\left(1+\tau^{-1}\right)^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}\log p\left(\alpha\right)}{\partial\alpha^{2}} & = & -\tau_{\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & -\left(\frac{\partial^{2}M}{\partial\alpha^{2}}\right)^{-1}\frac{\partial^{2}M_{MLE}}{\partial\alpha\partial t}\\
 & = & \tau^{-1}\left(\left(1+\tau^{-1}\right)^{-1}-\tau_{\alpha}\right)^{-1}\left(1+\tau^{-1}\right)^{-1}\\
\left(\frac{d\alpha}{dt}\right)^{-1} & = & \tau\left(\frac{1}{1+\tau^{-1}}-\tau_{\alpha}\right)\left(1+\tau^{-1}\right)\\
 & = & \left(\frac{\tau}{\tau+1}-\tau_{\alpha}\right)\left(\tau+1\right)\\
 & = & \left(\tau+\tau_{\alpha}\left(\tau+1\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
m\left(t,\alpha_{t}\right) & = & \mbe_{t_{i}}\left[\theta_{i}\vert\alpha_{t}\right]\\
\frac{dm}{dt} & = & \frac{\partial m}{\partial\alpha}\frac{d\alpha}{dt}+\frac{\partial m}{\partial t}\\
 & = & \left(\tau+\tau_{\alpha}\left(\tau+1\right)\right)^{-1}+\tau^{-1}\\
 & = & \frac{1}{\tau+\tau\tau_{\alpha}+\tau_{\alpha}}+\frac{1}{\tau}\\
 & = & \frac{\tau+\tau_{\alpha}\tau+\tau_{\alpha}+\tau}{\tau\left(\tau+\tau_{\alpha}\tau+\tau_{\alpha}\right)}\\
 & = & \frac{\tau+\tau_{\alpha}\tau+\tau_{\alpha}+\tau}{\tau\left(\tau+\tau_{\alpha}\tau+\tau_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau\tau_{\alpha}+\tau} & = & \textrm{target}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section

\end_layout

\begin_layout Standard
Let's take a classic EB problem, a simple Gamma-Poisson model.
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\vert\lambda_{i} & \sim & \textrm{Poisson}\left(\lambda_{i}\right)\\
\lambda_{i} & \sim & \textrm{Gamma}\left(\gamma,\beta\right)\\
\mbe\left[\lambda_{i}\right] & = & \frac{\gamma}{\beta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It is known that, marginally, 
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\vert\gamma,\beta & \sim & \textrm{NegBinom}\left(r,p\right)\\
\gamma & = & r\\
\beta & = & \frac{1-p}{p}\Rightarrow\\
p & = & \frac{1}{1+\beta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Typically, 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are chosen to maximize 
\begin_inset Formula $p\left(y\vert\gamma,\beta\right)$
\end_inset

 since 
\begin_inset Formula $\lambda_{i}$
\end_inset

 can be analytically marginalized out.
 However, if you knew 
\begin_inset Formula $\lambda_{i}$
\end_inset

, then 
\begin_inset Formula $y_{i}$
\end_inset

 becomes ancillary for 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
 In that case, a reasonable choice for 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 might be the MLE (or MAP with a prior):
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & -\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}+C\\
\hat{\gamma},\hat{\beta} & = & \textrm{argmax}\left\{ \frac{1}{N_{i}}\sum_{i}\left(-\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\right)\right\} \\
 & = & \textrm{argmax}\left\{ -\beta\frac{1}{N_{i}}\sum_{i}\lambda_{i}+\left(\gamma-1\right)\frac{1}{N}\sum_{i}\log\lambda_{i}\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is for a single sample, however, and we must choose a 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 to use for all the samples.
 A natural choice is the average MLE, where the average is taken over the
 posterior.
 In this case, 
\begin_inset Formula $\alpha=\left(\gamma,\beta\right)$
\end_inset

, 
\begin_inset Formula $\theta=\left(\lambda_{1},...,\lambda_{i},...,\lambda_{N_{i}}\right)$
\end_inset

 and
\begin_inset Formula 
\begin{eqnarray*}
\mu_{\alpha}\left(\lambda_{n},\gamma,\beta\right) & = & \left(\begin{array}{c}
\frac{1}{N_{i}}\sum_{i}\log\lambda_{in}\\
-\frac{1}{N_{i}}\sum_{i}\lambda_{in}
\end{array}\right)\\
\mu_{\alpha\alpha} & = & \left(\begin{array}{cc}
0 & 0\\
0 & 0
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Next,
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}+C\textrm{ (no dependence on }\alpha\textrm{)}\\
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\beta-\log\Gamma\left(\gamma\right)-\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log\rho\left(\lambda_{n},\gamma,\beta\right) & = & \log\prod_{i}p\left(y_{i}\vert\lambda_{i},\gamma,\beta\right)\\
 & = & \sum_{i}\log p\left(\lambda_{i}\vert\gamma,\beta\right)+C\\
 & = & N_{i}\gamma\log\beta-N_{i}\log\Gamma\left(\gamma\right)-\beta\sum_{i}\lambda_{i}+\left(\gamma-1\right)\sum_{i}\log\lambda_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\gamma}\log\rho\left(\lambda_{n},\gamma,\beta\right) & = & N_{i}\left(\log\beta-\psi\left(\gamma\right)\right)+\sum_{i}\log\lambda_{i}\\
 & = & N_{i}\left(\log\beta-\psi\left(\gamma\right)+\frac{1}{N_{i}}\sum_{i}\log\lambda_{i}\right)\\
\frac{\partial}{\partial\beta}\log\rho\left(\lambda_{n},\gamma,\beta\right) & = & N_{i}\frac{\gamma}{\beta}-\sum\lambda_{i}\\
 & = & N_{i}\left(\frac{\gamma}{\beta}-\frac{1}{N_{i}}\sum\lambda_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
These are the differences between the means as given by 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 and the sample means from the MCMC sample.
 Plugging in
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & \left[\frac{1}{n}\sum_{n}\frac{\partial}{\partial\alpha}\left.\log\rho\left(\alpha,\theta_{n}\right)\right|_{\hat{\alpha}}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)\right]^{-1}\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From this, we can see that
\begin_inset Formula 
\begin{eqnarray*}
\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)} & = & \widehat{\textrm{Cov}\left(\left(\begin{array}{c}
\lambda_{1n}\\
\vdots\\
\lambda_{in}\\
\vdots\\
\lambda_{N_{i}n}
\end{array}\right),\left(\begin{array}{c}
\frac{1}{N_{i}}\sum_{i}\log\lambda_{in}\\
-\frac{1}{N_{i}}\sum_{i}\lambda_{in}
\end{array}\right)\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
That is, the sample covariance between the individual draws and their own
 population means.
 And
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\alpha}\left.\log\rho\left(\alpha,\theta_{n}\right)\right|_{\hat{\alpha}}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right) & = & N_{i}\left(\begin{array}{c}
-\left(\psi\left(\gamma\right)-\log\beta\right)+\frac{1}{N_{i}}\sum_{i}\log\lambda_{i}\\
\frac{\gamma}{\beta}-\frac{1}{N_{i}}\sum\lambda_{i}
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{N_{i}}\sum_{i}\log\lambda_{in} & -\frac{1}{N_{i}}\sum_{i}\lambda_{in}\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is sensible -- the relationship between 
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

 is effected through 
\begin_inset Formula $\mu_{\alpha}$
\end_inset

, and we have the two covariances.
\end_layout

\begin_layout Section
Posterior moment calculations
\end_layout

\begin_layout Standard
Old writing
\end_layout

\begin_layout Standard
For the moment, assume that 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 is chosen as a function of some posterior moments, 
\begin_inset Formula $\mu$
\end_inset

, which are esimated with the draws 
\begin_inset Formula $\theta_{n}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha} & = & \underset{\alpha}{\textrm{argmax}}\left\{ \sum_{n}\mu\left(\theta_{n},\alpha\right)\right\} \\
0 & = & \sum_{n}\frac{\partial\mu\left(\theta_{n},\hat{\alpha}\right)}{\partial\alpha}:=\sum_{n}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It will be convenient to define
\begin_inset Formula 
\begin{eqnarray*}
\rho\left(\alpha,\theta\right) & := & p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)\\
\frac{\partial\rho\left(\alpha,\theta_{n}\right)}{\partial\alpha} & =: & \rho_{\alpha}\left(\alpha,\theta_{n}\right)\\
\frac{\partial\rho_{\alpha}\left(\alpha,\theta_{n}\right)}{\partial\alpha} & =: & \rho_{\alpha\alpha}\left(\alpha,\theta_{n}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $\alpha$
\end_inset

, the posterior of 
\begin_inset Formula $\theta$
\end_inset

 is given by
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta\vert x,\alpha\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\alpha\right)p\left(\theta\right)}{\int p\left(x\vert\theta,\alpha\right)p\left(\alpha\right)p\left(\theta\right)d\theta}\\
 & = & \frac{\rho\left(\theta,\alpha\right)}{\int\rho\left(\theta,\alpha\right)d\theta}\\
 & = & \frac{\rho\left(\theta,\alpha\right)}{c\left(\alpha\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Where it will also be covenient to define
\begin_inset Formula 
\begin{eqnarray*}
c\left(\alpha\right) & := & \int\rho\left(\theta,\alpha\right)d\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Next, suppose we had changed 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 to 
\begin_inset Formula $\tilde{\alpha}$
\end_inset

.
 We can use importance sampling to estimate the change in the MCMC sample
 with weights given by
\begin_inset Formula 
\begin{eqnarray*}
w_{n} & = & \frac{p\left(\theta_{n}\vert\tilde{\alpha}\right)}{p\left(\theta_{n}\vert\hat{\alpha}\right)}\\
 & = & \frac{c\left(\hat{\alpha}\right)\rho\left(\tilde{\alpha},\theta_{n}\right)}{c\left(\tilde{\alpha}\right)\rho\left(\hat{\alpha},\theta_{n}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In order to estimate the covariance of 
\begin_inset Formula $\theta$
\end_inset

, we need to consider the tilted likelihood:
\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(\theta\vert x\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)\exp\left(t\theta\right)}{p_{t}\left(x\right)}\\
\rho_{t}\left(\theta,\alpha\right) & = & \rho\left(\theta,\alpha\right)\exp\left(t\theta\right)\\
c_{t}\left(\alpha\right) & = & \int\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)d\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then conditional on the MCMC samples, which were drawn from 
\begin_inset Formula $p_{t}$
\end_inset

 with 
\begin_inset Formula $t=0$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & = & \underset{\alpha}{\textrm{argmax}}\left\{ \sum_{n}w_{t,n}\mu\left(\theta_{n},\alpha\right)\right\} \\
w_{t,n} & = & \frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha\right)}\cdot\frac{\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial t}\left.\frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha_{t}\right)}\right|_{t=0} & = & -\left.\frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha_{t}\right)^{2}}\int\theta\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)d\theta\right|_{t=0}\\
 & = & -\mbe_{\theta\vert x,\alpha}\left[\theta\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial t}\left.\frac{\rho\left(\theta,\alpha_{t}\right)\exp\left(t\theta\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\right|_{t=0} & = & \theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
To find the covariance of 
\begin_inset Formula $\theta$
\end_inset

, we need to determine
\begin_inset Formula 
\begin{eqnarray*}
m & := & \frac{1}{n}\sum_{n}\theta_{n}\\
m_{t} & := & \frac{1}{n}\sum_{n}w_{t,n}\theta_{n}\\
\hat{\Sigma}_{\theta} & = & \frac{dm_{t}}{dt}\\
 & = & \frac{1}{n}\sum_{n}\frac{dw_{t,n}}{dt}\theta_{n}\\
 & = & \frac{1}{n}\sum_{n}\left.\left(\frac{\partial w_{t,n}}{\partial\alpha}\frac{d\alpha}{dt}+\frac{\partial w_{t,n}}{\partial t}\right)\right|_{t=0}\theta_{n}
\end{eqnarray*}

\end_inset

Plugging in from above,
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial w_{t,n}}{\partial t}\right|_{t=0} & = & \left.\frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha\right)}\cdot\frac{\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\right|_{t=0}\\
 & = & \left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\\
\left.\frac{\partial w_{t,n}}{d\alpha}\right|_{t=0} & = & \frac{\rho_{\alpha}\left(\alpha,\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\\
 & = & \left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \frac{1}{n}\sum_{n}\left.\left(\left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}\frac{d\alpha}{dt}+\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\right)\right|_{t=0}\theta_{n}\\
 & = & \frac{1}{n}\sum_{n}\left.\left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}\frac{d\alpha}{dt}\right|_{t=0}\theta_{n}+\frac{1}{n}\sum_{n}\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)^{T}\\
 & = & \frac{1}{n}\sum_{n}\left.\left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}\frac{d\alpha}{dt}\right|_{t=0}\theta_{n}+\widehat{\textrm{Cov}\left(\theta\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that if 
\begin_inset Formula $\frac{d\alpha}{dt}=0$
\end_inset

 -- that is, if the estimate for 
\begin_inset Formula $\alpha$
\end_inset

 does not depend on the tilting of the likelihood -- then the estimated
 covariance of 
\begin_inset Formula $\theta$
\end_inset

 is simply the MCMC sample covariance.
\end_layout

\begin_layout Standard
Finally, bringing in the equations for 
\begin_inset Formula $\alpha_{t}$
\end_inset

, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
0 & = & \sum_{n}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Differnetiating both sides with respect to 
\begin_inset Formula $t$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
0 & = & \sum_{n}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\\
 & = & \sum_{n}\left(\frac{\partial}{\partial\alpha}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\frac{d\alpha}{dt}+\frac{\partial}{\partial t}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\\
 & = & \sum_{n}\left(\left(\frac{\rho_{\alpha}\left(\hat{\alpha},\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)+w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\frac{d\alpha}{dt}+\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Scaling and evaluating at 
\begin_inset Formula $t=0$
\end_inset

 gives
\begin_inset Formula 
\begin{eqnarray*}
0 & = & \left[\frac{1}{n}\sum_{n}\left(\frac{\rho_{\alpha}\left(\hat{\alpha},\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)+\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\right]\frac{d\alpha}{dt}+\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus, solving for 
\begin_inset Formula $\frac{d\alpha}{dt}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & \left[\frac{1}{n}\sum_{n}\left(\frac{\rho_{\alpha}\left(\hat{\alpha},\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)+\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\right]^{-1}\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It is convenient to use the fact that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\rho}{\partial\alpha} & = & \rho\frac{\partial\log\rho}{\partial\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & \left[\frac{1}{n}\sum_{n}\left(\frac{\partial}{\partial\alpha}\left.\log\rho\left(\alpha,\theta_{n}\right)\right|_{\hat{\alpha}}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)+\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\right]^{-1}\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Although cumbersome, this is a linear system only as big as the moment condition
s on 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\end_body
\end_document
