#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbe}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Part
Standard Empirical Bayes
\end_layout

\begin_layout Section
Model and problem statement
\end_layout

\begin_layout Standard
Given a parameter 
\begin_inset Formula $\theta$
\end_inset

, a meta-parameter 
\begin_inset Formula $\alpha$
\end_inset

, and data 
\begin_inset Formula $x$
\end_inset

, we want the posterior:
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta,\alpha\vert x\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)d\theta d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We will do empirical Bayes on 
\begin_inset Formula $\alpha$
\end_inset

 and MCMC on 
\begin_inset Formula $\theta$
\end_inset

.
 Specifically, given
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha} & := & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)d\theta\right\} \\
\theta_{n} & \sim & p\left(\theta\vert x,\hat{\alpha}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
A common question is how uncertainty in 
\begin_inset Formula $\alpha$
\end_inset

 would translate into uncertainty in 
\begin_inset Formula $\theta$
\end_inset

 if it were not fixed at 
\begin_inset Formula $\hat{\alpha}$
\end_inset

.
 We can answer this question with linear response and importance sampling
 by conditioning on the MCMC sample.
\end_layout

\begin_layout Section
Re-thinking the derivation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log\mbe\left[e^{\theta t}\right] & = & \log\frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)d\theta d\alpha}\\
\frac{d}{dt}\log\mbe\left[e^{\theta t}\right] & = & \frac{d}{dt}\log\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha\\
 & = & \frac{\frac{d}{dt}\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\theta e^{t\theta}d\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}\\
 & = & \mbe\left[\theta\vert x,t\right]\\
 & \approx & \mbe\left[\theta\vert x,\hat{\alpha}_{t},t\right]\\
\frac{d^{2}}{dtdt^{T}}\log\mbe\left[e^{\theta t}\right] & = & \frac{d}{dt^{T}}\frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\theta e^{t\theta}d\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\theta\theta^{T}e^{t\theta}d\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}-\\
 &  & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\theta^{T}e^{t\theta}d\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}\mbe\left[\theta\vert x,\alpha,t\right]-\\
 & = & \mbe\left[\theta\theta^{T}\vert x,\alpha,t\right]-\mbe\left[\theta\vert x,\alpha,t\right]\mbe\left[\theta\vert x,\alpha,t\right]^{T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Ok, our approximation is
\begin_inset Formula 
\begin{eqnarray*}
\frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\theta e^{t\theta}d\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha} & \approx & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)\theta e^{t\theta}d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)e^{t\theta}d\theta}=\mbe\left[\theta\vert x,\alpha,t\right]\\
\\
\textrm{Numerator:}\\
\frac{d}{dt^{T}}\int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)\theta e^{t\theta}d\theta & = & \int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)\theta\theta^{T}e^{t\theta}d\theta+\int p\left(x\vert\theta\right)\left.\frac{\partial p\left(\theta\vert\alpha\right)}{\partial\alpha}\frac{d\alpha}{dt^{T}}\right|_{\alpha=\hat{\alpha}_{t}}\theta e^{t\theta}d\theta\\
 & = & \int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)\theta\theta^{T}e^{t\theta}d\theta+\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{t\theta}\theta\left.\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha}\right|_{\alpha=\hat{\alpha}_{t}}d\theta\frac{d\hat{\alpha}_{t}}{dt^{T}}\\
\\
\textrm{Denominator:}\\
\frac{d}{dt}\int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)e^{t\theta}d\theta & = & \int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)e^{t\theta}\theta d\theta+\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{t\theta}\left.\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha}\right|_{\alpha=\hat{\alpha}_{t}}d\theta\frac{d\hat{\alpha}_{t}}{dt}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note of course also that
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta\vert x,\alpha\right] & = & \int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)\theta d\theta\\
\frac{\partial}{\partial\alpha^{T}}\mbe\left[\theta\vert x,\alpha\right] & = & \int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)\theta\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha}d\theta\\
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt}\left.\mbe\left[\theta\vert x,\alpha,t\right]\right|_{t=0} & = & \mbe\left[\theta\theta^{T}\vert x,\hat{\alpha}_{t}\right]+\mbe\left[\theta\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha}\vert x,\hat{\alpha}_{t}\right]\frac{d\hat{\alpha}_{t}}{dt^{T}}-\\
 &  & \mbe\left[\theta\vert x,\hat{\alpha}_{t}\right]\left(\int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)d\theta\right)^{-1}\left(\int p\left(x\vert\theta\right)p\left(\theta\vert\hat{\alpha}_{t}\right)\theta^{T}d\theta+\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)\left.\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha^{T}}\right|_{\alpha=\hat{\alpha}_{t}}d\theta\frac{d\hat{\alpha}_{t}}{dt^{T}}\right)\\
 & = & \mbe\left[\theta\theta^{T}\vert x,\hat{\alpha}_{t}\right]+\mbe\left[\theta\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha^{T}}\vert x,\hat{\alpha}_{t}\right]\frac{d\hat{\alpha}_{t}}{dt^{T}}-\mbe\left[\theta\vert x,\hat{\alpha}_{t}\right]\left(\mbe\left[\theta\vert x,\hat{\alpha}_{t}\right]+\mbe\left[\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha^{T}}\vert x,\hat{\alpha}_{t}\right]\frac{d\hat{\alpha}_{t}}{dt^{T}}\right)\\
 & = & \mbe\left[\theta\theta^{T}\vert x,\hat{\alpha}_{t}\right]-\mbe\left[\theta\vert x,\hat{\alpha}_{t}\right]\mbe\left[\theta\vert x,\hat{\alpha}_{t}\right]+\left(\mbe\left[\theta\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha^{T}}\vert x,\hat{\alpha}_{t}\right]-\mbe\left[\theta\vert x,\hat{\alpha}_{t}\right]\mbe\left[\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha^{T}}\vert x,\hat{\alpha}_{t}\right]\right)\frac{d\hat{\alpha}_{t}}{dt^{T}}\\
 & = & \textrm{Var}\left(\theta\vert x,\hat{\alpha}_{t}\right)+\textrm{Cov}\left(\theta,\frac{\partial\log p\left(\theta\vert\alpha\right)}{\partial\alpha^{T}}\vert x,\hat{\alpha}_{t}\right)\frac{d\hat{\alpha}_{t}}{dt^{T}}\\
 & = & \textrm{Var}\left(\theta\vert x,\hat{\alpha}_{t}\right)+\left.\frac{\partial}{\partial\alpha}\mbe\left[\theta\vert x,\alpha\right]\right|_{\alpha=\hat{\alpha}_{t}}\frac{d\hat{\alpha}_{t}}{dt^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Which matches what was done below.
\end_layout

\begin_layout Section
Derivation
\end_layout

\begin_layout Standard
Suppose we have a tilted posterior
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta,\alpha\vert x,t\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}d\theta d\alpha}\\
 & = & \exp\left(\ell\left(\theta,\alpha,t\right)-c\left(t\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where for notational convenience we have written
\begin_inset Formula 
\begin{eqnarray*}
\ell\left(\theta,\alpha,t\right) & = & \log\left(p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}\right)\\
c\left(t\right) & = & \log\left(\int\exp\left(\ell\left(\theta,\alpha,t\right)\right)d\theta d\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
with implicit dependence on 
\begin_inset Formula $x$
\end_inset

.
 Then
\begin_inset Formula 
\begin{eqnarray*}
\Sigma_{\theta}:=\frac{d}{dt^{T}}\left.\mbe_{p}\left[\theta\vert x,t\right]\right|_{t=0} & = & Cov_{p}\left(\theta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Under the assumption that the Empirical Bayes approximation is good, then
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{p}\left[\theta\vert x,t\right] & \approx & \mbe_{p}\left[\theta\vert x,\alpha,t\right]\Rightarrow\\
\hat{\Sigma}_{\theta} & := & \frac{d}{dt^{T}}\mbe_{p}\left[\theta\vert x,\alpha,t\right]\approx\frac{d}{dt^{T}}\mbe_{p}\left[\theta\vert x,t\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\alpha$
\end_inset

 depends on 
\begin_inset Formula $t$
\end_inset

 since, as a function of 
\begin_inset Formula $t$
\end_inset

, we have
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & := & \underset{\alpha}{\textrm{argmax}}\left\{ \log p\left(\alpha\vert x,t\right)\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)e^{t\theta}d\theta-\log\int\int p\left(x\vert\theta,\alpha'\right)p\left(\theta\right)p\left(\alpha'\right)e^{t\theta}d\theta d\alpha'\right\} \\
\hat{\alpha} & = & \hat{\alpha}_{0}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note at this point that 
\begin_inset Formula $\hat{\alpha}_{t}$
\end_inset

 is a function of 
\begin_inset Formula $t$
\end_inset

, and 
\begin_inset Formula $\mbe_{p}\left[\theta\vert x,\hat{\alpha}_{t},t\right]$
\end_inset

 is a function of both 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

.
 Taking the total derivative,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \left.\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha,t\right]}{\partial\alpha^{T}}\frac{d\alpha_{t}}{dt^{T}}\right|_{\alpha=\hat{\alpha}_{0},t=0}+\left.\frac{\partial\mbe_{p}\left[\theta\vert x,\hat{\alpha}_{0},t\right]}{\partial t}\right|_{t=0}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In order to evaluate 
\begin_inset Formula $\frac{d}{dt^{T}}\mbe_{p}\left[\theta\vert x,\alpha_{t},t\right]$
\end_inset

 we will need two terms: 
\begin_inset Formula $\frac{\partial}{\partial\alpha^{T}}\mbe_{p}\left[\theta\vert x,\alpha\right]$
\end_inset

 and 
\begin_inset Formula $\frac{d\alpha_{t}}{dt}$
\end_inset

.
 Note that 
\begin_inset Formula $\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha,t\right]}{\partial t}=\textrm{Var}\left(\theta\vert x,\alpha\right)$
\end_inset

, so this has the form of the conditional variance plus a correction term.
\end_layout

\begin_layout Subsection
\begin_inset Formula $d\alpha/dt$
\end_inset


\end_layout

\begin_layout Standard
First, we will calculate 
\begin_inset Formula $d\alpha/dt$
\end_inset

.
 Recall that
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & := & \underset{\alpha}{\textrm{argmax}}\left\{ p\left(\alpha\vert x,t\right)\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \frac{p\left(x\vert\alpha,t\right)p\left(\alpha\right)}{\int p\left(x\vert\alpha',t\right)p\left(\alpha'\right)d\alpha'}\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \frac{\int p\left(x\vert\alpha\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\right)d\theta}{\int\int p\left(x\vert\alpha'\right)p\left(\theta'\vert\alpha',t\right)p\left(\alpha'\right)d\alpha'd\theta'}\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\right)d\theta-\log\int\int p\left(x\vert\theta,\alpha'\right)p\left(\theta\vert\alpha',t\right)p\left(\alpha'\right)d\theta d\alpha'\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Where
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta\vert\alpha,t\right) & = & \frac{p\left(\theta\vert\alpha\right)e^{\theta^{T}t}}{\int p\left(\theta'\vert\alpha\right)e^{\theta^{T}t}d\theta'}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Since
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(x\vert\alpha,t\right) & = & \frac{\int p\left(x,\theta,\alpha\vert t\right)d\theta}{p\left(\alpha\vert t\right)}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}d\theta}{\int\int\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta d\alpha}\frac{\int\int\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta d\alpha}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}d\theta}{\int\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}{\int\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}{\int\left(\int p\left(x\vert\theta\right)dx\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}\\
 & = & \int p\left(x\vert\theta\right)\frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'}d\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus in the first term of the marginalized formula it suffices to increase
 by 
\begin_inset Formula $t$
\end_inset

 the natural parameter in front of the natural sufficient statistic 
\begin_inset Formula $\theta$
\end_inset

 in the distribution 
\begin_inset Formula $p_{t}\left(\theta\vert\alpha\right)$
\end_inset

 for all calculations.
 That is, we can use all the original posterior formulas but replacing 
\begin_inset Formula $p\left(\theta\vert\alpha\right)$
\end_inset

 with 
\begin_inset Formula $p\left(\theta\vert\alpha,t\right)$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta\vert\alpha,t\right) & = & \frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Let us define 
\begin_inset Formula 
\begin{eqnarray*}
M\left(\alpha,t\right) & := & \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\right)d\theta-\log\int\int p\left(x\vert\theta,\alpha'\right)p\left(\theta\vert\alpha',t\right)p\left(\alpha'\right)e^{t\theta}d\theta d\alpha'
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In many EB applications (e.g.
 example below), the first term in 
\begin_inset Formula $M\left(\alpha,t\right)$
\end_inset

 has a closed form since 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 are conjugate and the marginal is a ratio of normalizing constants.
 We will define this log normalizing constant as
\begin_inset Formula 
\begin{eqnarray*}
C_{\alpha}\left(t\right) & := & \log\int\int p\left(x\vert\theta,\alpha'\right)p\left(\theta\vert\alpha'\right)p\left(\alpha'\right)e^{t\theta}d\theta d\alpha'
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $C_{\alpha}\left(t\right)$
\end_inset

 is a function only of 
\begin_inset Formula $t$
\end_inset

 and not of 
\begin_inset Formula $\alpha$
\end_inset

, so it is irrelevant to the optimization.
\end_layout

\begin_layout Standard
Plugging in,
\begin_inset Formula 
\begin{eqnarray*}
M\left(\alpha,t\right) & = & \log\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)d\theta+\log p\left(\alpha\right)-C_{\alpha}\left(t\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta\vert x,\alpha,t\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)d\theta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that the first term in 
\begin_inset Formula $M\left(\alpha,t\right)$
\end_inset

 is the log normalizer of the posterior of 
\begin_inset Formula $p\left(\theta\vert x,\alpha,t\right)$
\end_inset

.
 And
\begin_inset Formula 
\begin{eqnarray*}
\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)d\theta & = & \int p\left(x\vert\theta\right)\frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}d\theta\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}{\int p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
is the ratio of normalizers of the tilted posterior and prior on 
\begin_inset Formula $\theta$
\end_inset

.
 The derivative 
\begin_inset Formula $\partial^{2}/\partial\alpha\partial t^{T}\log\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta$
\end_inset

 gives us 
\begin_inset Formula $\partial/\partial\alpha\mbe\left[\theta\vert x,\alpha\right]$
\end_inset

.
 The second, however, seems to be unaccounted for and may lead the covariance
 estimate to be asymmetric.
 It gives 
\begin_inset Formula $\partial/\partial\alpha\mbe\left[\theta\vert\alpha\right]$
\end_inset

, which will also depend much more strongly on 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Standard
This is choosing 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 to maximize the difference between two CGFS:
\begin_inset Formula 
\begin{eqnarray*}
M\left(\alpha,t\right) & = & \log\mbe\left[e^{\theta t}\vert x,\alpha\right]-\log\mbe\left[e^{\theta t}\vert\alpha\right]+\log p\left(\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Strange, dig that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial t}\log\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)d\theta & = & \frac{\partial}{\partial t}\left(\log\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta-\log\int p\left(\theta\vert\alpha\right)e^{\theta t}d\theta\right)\\
 & = & \mbe\left[\theta\vert x,\alpha,t\right]-\mbe\left[\theta\vert\alpha,t\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
But since 
\begin_inset Formula $\log\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)d\theta$
\end_inset

 is the log normalizer of 
\begin_inset Formula $p\left(\theta\vert x,\alpha,t\right)$
\end_inset

, one might expect instead
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial t}\log\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)d\theta & = & \mbe\left[\theta\vert x,\alpha,t\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
....
\end_layout

\begin_layout Standard
For any 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 is chosen so that
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial M}{\partial\alpha}\right|_{t,\alpha_{t}} & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
we can differentiate using the chain rule, giving
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\frac{d\alpha_{t}}{dt}+\frac{\partial^{2}M}{\partial\alpha\partial t} & = & 0\Rightarrow\\
\frac{d\alpha}{dt} & = & -\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that since 
\begin_inset Formula $M$
\end_inset

 is maximized, its Hessian will be negative definite.
 Note that 
\begin_inset Formula $C_{\alpha}\left(t\right)$
\end_inset

 doesn't enter this calculation since it is not a function of 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Standard
Finally, observe that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial^{2}M}{\partial\alpha\partial t^{t}}\right|_{t=0} & = & \left.\frac{\partial^{2}}{\partial\alpha\partial t^{t}}\log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)d\theta\right|_{t=0}\\
\left.\frac{\partial M}{\partial t}\right|_{t=0} & = & \frac{\left.\frac{\partial}{\partial t^{T}}\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)d\theta\right|_{t=0}}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)d\theta}\\
 & = & \frac{\int p\left(x\vert\theta,\alpha\right)\frac{\partial}{\partial t^{T}}p\left(\theta\vert\alpha,t\right)d\theta}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)d\theta}\\
\frac{\partial}{\partial t^{T}}p\left(\theta\vert\alpha,t\right) & = & \frac{\partial}{\partial t^{T}}\frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}\\
 & = & \frac{\theta p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta\vert\alpha\right)e^{\theta t}d\theta}-\frac{p\left(\theta\vert\alpha\right)e^{\theta t}\int p\left(\theta\vert\alpha\right)e^{\theta t}\theta d\theta}{\left(\int p\left(\theta\vert\alpha\right)e^{\theta t}d\theta\right)^{2}}\\
\left.\frac{\partial}{\partial t^{T}}p\left(\theta\vert\alpha,t\right)\right|_{t=0} & = & p\left(\theta\vert\alpha\right)\left(\theta-\mbe\left[\theta\vert\alpha\right]\right)\Rightarrow\\
\left.\frac{\partial M}{\partial t}\right|_{t=0} & = & \frac{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)\left(\theta-\mbe\left[\theta\vert\alpha\right]\right)d\theta}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)d\theta}\Rightarrow\\
 & = & \mbe\left[\theta\vert\alpha,x\right]-\mbe\left[\theta\vert\alpha\right]\\
 & \ne & \mbe\left[\theta\vert\alpha,x\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset Formula $\partial\mbe_{p}\left[\theta\vert x,\alpha,t\right]/\partial\alpha$
\end_inset


\end_layout

\begin_layout Standard
This derivative can sometimes be calculated exactly, or it can be evaluated
 with MCMC draws by differentiating under the integral.
\end_layout

\begin_layout Subsubsection
With analytic expectation
\end_layout

\begin_layout Standard
In certain cases (e.g.
 conjugate exponential families), 
\begin_inset Formula $\mbe_{p}\left[\theta\vert x,\alpha,t\right]=m\left(\alpha,t\right)$
\end_inset

 is a known function of 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray*}
\Sigma_{\theta} & = & \frac{dm}{dt^{T}}\\
 & = & \frac{\partial m}{\partial\alpha^{T}}\frac{d\alpha}{dt^{T}}+\frac{\partial m}{\partial t}\\
 & = & -\frac{\partial m}{\partial\alpha^{T}}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}+\frac{\partial m}{\partial t}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\partial m/\partial t$
\end_inset

 is the covariance for fixed 
\begin_inset Formula $\alpha$
\end_inset

.
 As shown above, 
\begin_inset Formula $\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}\ne\frac{\partial m^{T}}{\partial\alpha}$
\end_inset

, so this not is guaranteed to be positive definite and symmetric.
\end_layout

\begin_layout Subsubsection
From MCMC draws
\end_layout

\begin_layout Standard
You can estimate this parametric sensitivity by differentiating under the
 intergral and calculating the covariance with MCMC draws.
\end_layout

\begin_layout Section
Gamma Poisson Example
\end_layout

\begin_layout Standard
Let's take a classic EB problem, a simple Gamma-Poisson model.
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\vert\lambda_{i} & \sim & \textrm{Poisson}\left(\lambda_{i}\right)\\
\lambda_{i} & \sim & \textrm{Gamma}\left(\gamma,\beta\right)\\
\mbe\left[\lambda_{i}\right] & = & \frac{\gamma}{\beta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Additionally, we may put gamma priors on 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 (at the risk of having too many gammas around):
\begin_inset Formula 
\begin{eqnarray*}
\gamma & \sim & \textrm{Gamma}\left(a_{\gamma},b_{\gamma}\right)\\
\beta & \sim & \textrm{Gamma}\left(a_{\beta},b_{\beta}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In the above notation, we have
\begin_inset Formula 
\begin{eqnarray*}
x & = & y\\
\theta & = & \left(\lambda_{1},...,\lambda_{K}\right)^{T}\\
\alpha & = & \left(\gamma,\beta\right)^{T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Variance of 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard
This can be marginalized exactly as a function of 
\begin_inset Formula $t$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}-\log y_{i}!\\
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\beta-\log\Gamma\left(\gamma\right)-\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log p\left(y,\lambda\vert\gamma,\beta\right) & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\log y_{i}!\\
 &  & +\sum_{i}\left[-\left(\beta+1\right)\lambda_{i}+\left(\gamma+y_{i}-1\right)\log\lambda_{i}+\left(\gamma+y_{i}\right)\log\left(\beta+1\right)-\log\Gamma\left(\gamma+y_{i}\right)\right]\\
 &  & -\sum_{i}\left[\left(\gamma+y_{i}\right)\log\left(\beta+1\right)-\log\Gamma\left(\gamma+y_{i}\right)\right]\Rightarrow\\
\log p\left(y\vert\gamma,\beta\right) & = & \sum_{i}\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!\\
\log p\left(y\vert\gamma,\beta,t\right) & = & \sum_{i}\gamma\log\left(\beta-t_{i}\right)-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1-t_{i}\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
Marginal likelihood
\end_layout

\begin_layout Standard
We choose 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 to maximize the marginal likelihood, so that
\begin_inset Formula 
\begin{eqnarray*}
M & = & \log p_{t}\left(y\vert\gamma,\beta\right)\\
 & = & \sum_{i}\gamma\log\left(\beta-t_{i}\right)-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1-t_{i}\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The derivatives are then
\begin_inset Formula 
\begin{eqnarray*}
0=\frac{\partial}{\partial\gamma}\log p_{t} & = & \sum_{i}\log\left(\beta-t_{i}\right)-N\psi\left(\gamma\right)-\sum_{i}\log\left(\beta+1-t_{i}\right)+\sum_{i}\psi\left(\gamma+y_{i}\right)\\
0=\frac{\partial}{\partial\beta}\log p_{t} & = & \sum_{i}\left(\frac{\gamma}{\beta-t_{i}}-\frac{\gamma+y_{i}}{1+\beta-t_{i}}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}} & = & N\left(\begin{array}{cc}
\frac{1}{N}\sum_{i}\left(\psi'\left(\gamma+y_{i}\right)-\psi'\left(\gamma\right)\right) & \frac{1}{N}\sum_{i}\left(\frac{1}{\beta-t_{i}}-\frac{1}{1+\beta-t_{i}}\right)\\
\frac{1}{N}\sum_{i}\left(\frac{1}{\beta-t_{i}}-\frac{1}{1+\beta-t_{i}}\right) & \frac{1}{N}\sum_{i}\left(\frac{\gamma+y_{i}}{\left(1+\beta-t_{i}\right)^{2}}-\frac{\gamma}{\left(\beta-t_{i}\right)^{2}}\right)
\end{array}\right)\\
\frac{\partial^{2}M}{\partial\alpha\partial t_{i}} & = & \left(\begin{array}{c}
\frac{1}{1+\beta-t_{i}}-\frac{1}{\beta-t_{i}}\\
\frac{\gamma}{\left(\beta-t_{i}\right)^{2}}-\frac{\gamma+y_{i}}{\left(1+\beta-t_{i}\right)^{2}}
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that here 
\begin_inset Formula $N$
\end_inset

 refers to observations within a sample, not the number of MCMC draws, which
 is denoted by 
\begin_inset Formula $D$
\end_inset

.
 Also, the leading 
\begin_inset Formula $N$
\end_inset

 in the Hessian cancels the leading 
\begin_inset Formula $N$
\end_inset

 in 
\begin_inset Formula $\ell_{\alpha,d}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
MAP estimates
\end_layout

\begin_layout Standard
Alternatively, we can take the MAP so that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(\gamma,\beta\vert y\right) & = & \frac{p_{t}\left(y\vert\gamma,\beta\right)p\left(\gamma,\beta\right)}{p_{t}\left(y\right)}\\
M_{MAP} & = & \log p_{t}\left(y\vert\gamma,\beta\right)+\log p\left(\gamma,\beta\right)-\log p_{t}\left(y\right)\\
 & = & M+\log p\left(\gamma,\beta\right)-\log p_{t}\left(y\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\log p_{t}\left(y\right)}{\partial\alpha} & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
so that we don't have to worry about the normalizing constant's dependence
 on 
\begin_inset Formula $t$
\end_inset

.
 Since
\begin_inset Formula 
\begin{eqnarray*}
\gamma & \sim & \textrm{Gamma}\left(a_{\gamma},b_{\gamma}\right)\\
\log p\left(\gamma\right) & = & -b_{\gamma}\gamma+\log\left(\gamma\right)\left(a_{\gamma}-1\right)+C\\
\frac{\partial^{2}\log p\left(\gamma\right)}{\partial\gamma^{2}} & = & -\frac{a_{\gamma}-1}{\gamma^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And similarly
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}\log p\left(\beta\right)}{\partial\beta^{2}} & = & -\frac{a_{\beta}-1}{\beta^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
MCMC weights
\end_layout

\begin_layout Standard
Here, we include the priors, to get
\begin_inset Formula 
\begin{eqnarray*}
\ell\left(\theta,\alpha,t\right) & = & \log p_{t}\left(y,\lambda\vert\gamma,\beta\right)+\log p\left(\gamma\right)+\log p\left(\beta\right)\\
 & = & \sum_{i}\gamma\log\left(\beta-t_{i}\right)-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1-t_{i}\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 &  & -b_{\gamma}\gamma+\left(a_{\gamma}-1\right)\log\gamma-b_{\beta}\beta+\left(b_{\beta}-1\right)\log\beta
\end{eqnarray*}

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
\ell_{\hat{\alpha},d} & = & \left(\begin{array}{c}
\frac{\partial}{\partial\gamma}\ell\left(\theta_{d},\hat{\alpha},0\right)\\
\frac{\partial}{\partial\beta}\ell\left(\theta_{d},\hat{\alpha},0\right)
\end{array}\right)\\
\frac{\partial}{\partial\gamma}\ell\left(\theta,\alpha,0\right) & = & N\left(\log\beta-\psi\left(\gamma\right)+\frac{1}{N}\sum_{i}\log\lambda_{i}-b_{\gamma}+\frac{a_{\gamma}-1}{\gamma}\right)\\
\frac{\partial}{\partial\beta}\ell\left(\theta,\alpha,0\right) & = & N\left(\frac{\gamma}{\beta}-\frac{1}{N}\sum\lambda_{i}-b_{\beta}+\frac{a_{\beta}-1}{\beta}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Exact moments
\end_layout

\begin_layout Standard
Instead of using MCMC, in this case we can use exact moments.
 Specifically,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}-\log y_{i}!\\
\log p_{t}\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\left(\beta+t_{i}\right)-\log\Gamma\left(\gamma\right)-\left(\beta-t_{i}\right)\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log p_{t}\left(\lambda_{i}\vert y_{i},\gamma,\beta\right) & = & \left(y_{i}+\gamma\right)\log\left(\beta+1+t_{i}\right)-\log\Gamma\left(y_{i}+\gamma\right)-\left(\beta+1-t_{i}\right)\lambda_{i}+\left(y_{i}+\gamma-1\right)\log\lambda_{i}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\mbe_{p_{t}}\left[\lambda_{i}\vert\alpha,y_{i}\right] & = & \frac{\gamma+y_{i}}{\beta+1-t_{i}}:=m_{i}\left(\alpha,t_{i}\right)\\
\left.\frac{\partial m_{i}}{\partial t_{i}}\right|_{t_{i}=0} & = & \frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}\\
\left.\frac{\partial m_{i}}{\partial\gamma}\right|_{t_{i}=0} & = & \frac{1}{1+\beta}\\
\left.\frac{\partial m_{i}}{\partial\beta}\right|_{t_{i}=0} & = & -\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}_{t}\left(\lambda_{i}\right) & = & -\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\begin{array}{c}
\frac{1}{1+\beta}-\frac{1}{\beta}\\
\frac{\gamma}{\beta^{2}}-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)+\frac{y_{i}+\gamma}{\left(\beta+1\right)^{2}}\\
 & = & -\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\begin{array}{c}
\frac{1}{1+\beta}\\
\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)-\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\begin{array}{c}
-\frac{1}{\beta}\\
\frac{\gamma}{\beta^{2}}
\end{array}\right)+\frac{y_{i}+\gamma}{\left(\beta+1\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Variance of 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
If we try to do the same trick as above,
\begin_inset Formula 
\begin{eqnarray*}
p\left(x,\theta,\alpha\right) & = & p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\\
p_{t}\left(x,\theta,\alpha\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}dxd\theta d\alpha}\\
p_{t}\left(x\vert\alpha\right) & = & \frac{\int p_{t}\left(x,\theta,\alpha\right)d\theta}{p_{t}\left(\alpha\right)}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\alpha t}dxd\theta}\\
 & = & \frac{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)d\theta}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)dxd\theta}\\
 & = & p\left(x\vert\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
A similar argument will apply to 
\begin_inset Formula $p_{t}\left(x,\theta\vert\alpha\right)$
\end_inset

.
 However, under 
\begin_inset Formula $t\theta$
\end_inset

 perturbations we will still have.
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left[\theta\vert x\right] & = & \mbe\left[\textrm{Var}\left[\theta\vert x,\alpha\right]\vert x\right]+\textrm{Var}\left(\mbe\left[\theta\vert x,\alpha\right]\vert x\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...from which we can infer (maybe) the moments of 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Subsection
Covariances as matrix products
\end_layout

\begin_layout Standard
Here, if the vector draws 
\begin_inset Formula $\lambda_{d}$
\end_inset

 are arranged columnwise:
\begin_inset Formula 
\begin{eqnarray*}
\Lambda & = & \left(\begin{array}{ccc}
\left(\lambda_{1}-\bar{\lambda}\right) & \cdots & \left(\lambda_{D}-\bar{\lambda}\right)\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...and we stack the products 
\begin_inset Formula $\ell_{\hat{\alpha},d}^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}$
\end_inset

 in rows, so that 
\begin_inset Formula 
\begin{eqnarray*}
Q & = & \left(\begin{array}{c}
\ell_{\hat{\alpha},1}^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}\\
\vdots\\
\ell_{\hat{\alpha},D}^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{array}\right)\\
 & = & \left(\begin{array}{c}
\ell_{\hat{\alpha},1}^{T}\\
\vdots\\
\ell_{\hat{\alpha},D}^{T}
\end{array}\right)\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then the covariance correction can be estimated with 
\begin_inset Formula $\frac{1}{D}\Lambda Q$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \widehat{\textrm{Cov}\left(\theta\right)}-\frac{1}{D}\Lambda Q
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Gamma moments
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
x & \sim & \textrm{Gamma}\left(\alpha,\beta\right)\\
p\left(x\right) & = & \frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}x^{\alpha-1}e^{-\beta x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[x^{k}\right] & = & \int\frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}x^{\alpha-1}e^{-\beta x}x^{k}dx\\
 & = & \frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}\frac{\Gamma\left(k+\alpha\right)}{\beta^{k+\alpha}}\int\frac{\beta^{k+\alpha}}{\Gamma\left(k+\alpha\right)}x^{k+\alpha-1}e^{-\beta x}dx\\
 & = & \frac{\Gamma\left(k+\alpha\right)}{\Gamma\left(\alpha\right)}\beta^{-k}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From the internet, the variance of the sample variance is given by
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(s^{2}\right) & = & \frac{\left(N-1\right)^{2}}{N^{3}}\mu_{4}-\frac{\left(N-1\right)\left(N-3\right)\mu_{2}^{2}}{N^{3}}\\
\mu_{4} & = & \mbe\left[\left(x-\mu\right)^{4}\right]\\
 & = & \mbe\left[x^{4}-4x^{3}\mu+6x^{2}\mu^{2}-4x\mu^{3}+\mu^{4}\right]\\
 & = & m_{4}-4m_{3}\mu+6m_{2}\mu^{2}-4\mu^{4}+\mu^{4}\\
 & = & m_{4}-4m_{3}\mu+6m_{2}\mu^{2}-3\mu^{4}\\
\mu_{2} & = & m_{2}-\mu^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then by the delta method
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(\sqrt{x}\right) & \approx & \frac{\textrm{Var}\left(x\right)}{4\mbe\left[x\right]}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Normal-Normal Example
\end_layout

\begin_layout Standard
Let's take an even more classic example:
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\theta_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,\tau^{-1}\right)\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\tau,\tau_{\alpha} &  & \textrm{are known}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(x_{i}\vert\theta_{i}\right) & = & -\frac{1}{2}\left(x_{i}-\theta_{i}\right)^{2}+C\\
 & = & -\frac{1}{2}x_{i}^{2}+x_{i}\theta_{i}-\frac{1}{2}\theta_{i}^{2}\\
\log p\left(\theta_{i}\vert\alpha\right) & = & -\frac{1}{2}\left(\tau\left(\theta_{i}-\alpha\right)^{2}-\log\tau\right)+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha-\frac{1}{2}\tau\alpha^{2}-\frac{1}{2}\log\tau+C\\
\log p\left(\alpha\right) & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
True posterior: integrating out 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
Here, we know that marginally over 
\begin_inset Formula $\alpha$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i} & \sim & \mathcal{N}\left(0,\tau_{\theta}^{-1}\right)\\
\tau_{\theta} & := & \left(\tau^{-1}+\tau_{\alpha}^{-1}\right)^{-1}\\
 & = & \frac{\tau\tau_{\alpha}}{\tau+\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\tau_{\theta}^{-1} & = & \tau^{-1}+\tau_{\alpha}^{-1}>\tau^{-1}\Rightarrow\\
\tau & > & \tau_{\theta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus the posterior is given by
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert x_{i}\right] & = & \frac{1}{\tau_{\theta}+1}\left(0\cdot\tau_{\theta}+x_{i}\right)\\
 & = & \frac{x_{i}}{\tau_{\theta}+1}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert x_{i} & \sim & \mathcal{N}\left(\frac{1}{\tau_{\theta}+1}\cdot x_{i},\left(\tau_{\theta}+1\right)^{-1}\right)\\
\frac{1}{\tau_{\theta}+1} & = & \frac{\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau+\tau\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is the variance we hope to recover with linear response.
 
\end_layout

\begin_layout Subsection
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset

 for Empirical Bayes
\end_layout

\begin_layout Standard
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,\tau_{x}^{-1}\right)\\
\tau_{x} & := & \left(1+\tau^{-1}\right)^{-1}=\frac{\tau}{1+\tau}\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\mbe\left[\alpha\vert x_{i}\right] & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that this is maximized at 
\begin_inset Formula $\hat{\alpha}=\frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}x_{i}$
\end_inset

.
 Thus the EB posterior of 
\begin_inset Formula $\theta_{i}$
\end_inset

 is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\hat{\alpha} & \sim & \mathcal{N}\left(\frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}x_{i},\tau^{-1}\right)\\
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\textrm{Var}\left(\theta_{i}\vert\hat{\alpha},x_{i}\right) & = & \left(1+\tau\right)^{-1}\\
\mbe\left[\theta_{i}\vert\hat{\alpha},x_{i}\right] & = & \frac{1}{1+\tau}\left(\frac{\tau_{x}\tau}{\tau_{\alpha}+\tau_{x}}+1\right)x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Dig that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\tau_{x}\tau}{\tau_{\alpha}+\tau_{x}} & = & \frac{\tau^{2}}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
\frac{\tau_{x}\tau}{\tau_{\alpha}+\tau_{x}}+1 & = & \frac{\tau^{2}+\tau_{\alpha}+\tau_{\alpha}\tau+\tau}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
 & = & \frac{\tau\left(\tau+\tau_{\alpha}\right)+\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
 & = & \frac{\left(1+\tau\right)\left(\tau+\tau_{\alpha}\right)}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
 & = & \frac{1+\tau}{1+\tau_{\theta}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert\hat{\alpha},x_{i}\right] & = & \frac{1}{1+\tau_{\theta}}x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Here, in contrast to the correct model with 
\begin_inset Formula $\alpha$
\end_inset

 marginalized out, 
\begin_inset Formula $\theta_{i}\vert\hat{\alpha},x_{i}$
\end_inset

 has an estimated variance that is too small since 
\begin_inset Formula $\tau>\tau_{m}$
\end_inset

.
 However, the shrinkage matches the marginal case exactly.
 The difference in the variances is given by
\begin_inset Formula 
\begin{eqnarray*}
\frac{1}{1+\tau_{\theta}}-\frac{1}{1+\tau} & = & \frac{\tau+\tau_{\alpha}}{\tau+\tau\tau_{\alpha}+\tau_{\alpha}}-\frac{1}{1+\tau}\\
 & = & \frac{\left(1+\tau\right)\left(\tau+\tau_{\alpha}\right)-\tau-\tau\tau_{\alpha}-\tau_{\alpha}}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)\left(1+\tau\right)}\\
 & = & \frac{\tau+\tau_{\alpha}+\tau^{2}+\tau\tau_{\alpha}-\tau-\tau\tau_{\alpha}-\tau_{\alpha}}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)\left(1+\tau\right)}\\
 & = & \frac{\tau^{2}}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)\left(1+\tau\right)}\\
 & = & \frac{\tau_{x}^{2}\left(1+\tau\right)}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)}\\
 & = & \frac{\tau_{x}^{2}\left(1+\tau\right)}{\left(\tau+\left(\tau+1\right)\tau_{\alpha}\right)}\\
 & = & \frac{\tau_{x}^{2}}{\frac{\tau}{\left(1+\tau\right)}+\tau_{\alpha}}\\
 & = & \frac{\tau_{x}^{2}}{\tau_{x}+\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Tilted model
\end_layout

\begin_layout Standard
Suppose we are interested in the variance of 
\begin_inset Formula $\theta_{i}$
\end_inset

.
 Then we perturb with 
\begin_inset Formula $t_{i}\theta_{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(x,\theta,\alpha,t\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)}{\int\int\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)dxd\theta d\alpha}\\
 & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)}{\int\int p\left(\theta\vert\alpha\right)\exp\left(t\theta\right)p\left(\alpha\right)d\theta d\alpha}\\
 & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)}{\int\int p\left(\theta\vert\alpha\right)\exp\left(t\theta\right)d\theta p\left(\alpha\right)d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta_{i},\alpha\vert t_{i}\right) & = & \frac{p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)}{\int\int p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(\alpha\vert t_{i}\right) & = & \frac{\int p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}}{\int\int p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}d\alpha}\\
 & = & \frac{p\left(\alpha\right)\int p\left(\theta_{i}\vert\alpha,t_{i}\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}}{\int p\left(\alpha\right)\int p\left(\theta_{i}\vert\alpha,t_{i}\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(\theta_{i}\vert\alpha,t_{i}\right) & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha-\frac{1}{2}\tau\alpha^{2}+t_{i}\theta_{i}+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\left(\alpha+\tau^{-1}t_{i}\right)\theta_{i}-\frac{1}{2}\tau\left(\alpha+\tau^{-1}t_{i}\right)^{2}+\frac{1}{2}\tau\left(\alpha+\tau^{-1}t_{i}\right)^{2}-\frac{1}{2}\tau\alpha^{2}+C\\
 & = & -\frac{1}{2}\tau\left(\theta_{i}-\left(\alpha+\tau^{-1}t_{i}\right)\right)^{2}+\frac{1}{2}\tau\left(\alpha+\tau^{-1}t_{i}\right)^{2}-\frac{1}{2}\tau\alpha^{2}+C\\
 & = & \log p\left(\theta_{i}\vert\alpha_{t},t_{i}\right)+\frac{1}{2}\tau\left(\alpha^{2}+2\tau^{-1}t_{i}\alpha+\tau^{-2}t_{i}^{2}\right)-\frac{1}{2}\tau\alpha^{2}+C\\
 & = & \log p\left(\theta_{i}\vert\alpha_{t},t_{i}\right)+t_{i}\alpha+C\\
\alpha_{t} & := & \alpha+\tau^{-1}t_{i}\\
\log p\left(\alpha\vert t_{i}\right) & = & \log p\left(\alpha\right)++t_{i}\alpha+C\\
 & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}+t_{i}\alpha+C\\
 & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}+\tau_{\alpha}\tau_{\alpha}^{-1}t_{i}\alpha+C\\
 & = & -\frac{1}{2}\tau_{\alpha}\left(\alpha-\tau_{\alpha}^{-1}t_{i}\right)^{2}+C
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\alpha\vert t_{i} & \sim & \mathcal{N}\left(\tau_{\alpha}^{-1}t_{i},\tau_{\alpha}^{-1}\right)\\
\theta_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau^{-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that, as expected,
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt_{i}}\mbe\left[\theta_{i}\vert\alpha,t_{i}\right] & = & \tau^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note also that
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Cov}\left(\theta_{i},\alpha\right) & = & \mbe\left[\theta_{i}\alpha\right]-\mbe\left[\theta_{i}\right]\mbe\left[\alpha\right]\\
 & = & \mbe\left[\alpha\mbe\left[\theta_{i}\vert\alpha\right]\right]-\mbe\left[\alpha^{2}\right]\\
 & = & \mbe\left[\alpha^{2}\right]-\mbe\left[\alpha^{2}\right]\\
 & = & \textrm{Var}\left(\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
and that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt_{i}}\mbe\left[\alpha\vert t_{i}\right] & = & \tau_{\alpha}^{-1}\\
 & = & \textrm{Var}\left(\alpha\right)\\
 & = & \textrm{Cov}\left(\theta_{i},\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...also as expected.
\end_layout

\begin_layout Subsubsection
Exact tilted posterior
\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $t_{i}$
\end_inset

 we can again integrate out 
\begin_inset Formula $\alpha$
\end_inset

.
 First observe that
\begin_inset Formula 
\begin{eqnarray*}
\tau_{\alpha}^{-1}+\tau^{-1} & = & \frac{1}{\tau_{\alpha}}+\frac{1}{\tau}\\
 & = & \frac{\tau+\tau_{\alpha}}{\tau_{\alpha}\tau}\\
 & = & \tau_{\theta}^{-1}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert t_{i} & \sim & \mathcal{N}\left(\tau_{\theta}^{-1}t_{i},\tau_{\theta}^{-1}\right)\\
x_{i}\vert\theta_{i},t_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\Rightarrow\\
\mbe\left[\theta_{i}\vert x_{i},t_{i}\right] & = & \frac{1}{1+\tau_{\theta}}\left(x_{i}+\tau_{\theta}\tau_{\theta}^{-1}t_{i}\right)\\
 & = & \frac{1}{1+\tau_{\theta}}\left(x_{i}+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Observe that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt_{i}}\left.\mbe\left[\theta_{i}\vert x_{i},t_{i}\right]\right|_{t_{i}=0} & = & \frac{1}{1+\tau_{\theta}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
as expected.
\end_layout

\begin_layout Subsubsection
EB with a tilted posterior
\end_layout

\begin_layout Standard
Now we will fix 
\begin_inset Formula $\alpha$
\end_inset

 and evaluate the EB posterior approximation as a function of 
\begin_inset Formula $t_{i}$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau^{-1}\right)\\
x_{i}\vert\theta_{i},t_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\mbe\left[\theta_{i}\vert x_{i},t_{i},\alpha\right] & = & \frac{1}{1+\tau}\left(x_{i}+\tau\left(\alpha+\tau^{-1}t_{i}\right)\right)\\
 & = & \frac{1}{1+\tau}\left(x_{i}+\tau\alpha+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
m & := & \mbe\left[\theta_{i}\vert\alpha,t_{i},x_{i}\right]\\
\frac{\partial m}{\partial t_{i}} & = & \frac{1}{1+\tau}\\
\frac{\partial m}{\partial\alpha} & = & \frac{\tau}{1+\tau}=\tau_{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Formula $\hat{\alpha}$
\end_inset

 as a function of 
\begin_inset Formula $t_{i}$
\end_inset


\end_layout

\begin_layout Standard
Next, we see that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\alpha\vert t_{i} & \sim & \mathcal{N}\left(\tau_{\alpha}^{-1}t_{i},\tau_{\alpha}^{-1}\right)\\
x_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau_{x}^{-1}\right)\Rightarrow\\
x_{i}-\tau^{-1}t_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha,\tau_{x}^{-1}\right)\\
\mbe\left[\alpha\vert x_{i},t_{i}\right] & = & \frac{1}{\tau_{\alpha}+\tau_{x}}\left(\tau_{\alpha}\tau_{\alpha}^{-1}t_{i}+\tau_{x}\left(x_{i}-\tau^{-1}t_{i}\right)\right)\\
 & = & \frac{1}{\tau_{\alpha}+\tau_{x}}\left(t_{i}+\tau_{x}x_{i}-\tau_{x}\tau^{-1}t_{i}\right)\\
 & = & \frac{1}{\tau_{\alpha}+\tau_{x}}\left(\tau_{x}x_{i}+\left(1-\frac{1}{1+\tau}\right)t_{i}\right)\\
 & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}\left(x_{i}+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As a function of 
\begin_inset Formula $\alpha$
\end_inset

, this is maximized at
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha} & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}\left(x_{i}+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\hat{\alpha}}{dt} & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Putting this together, we get that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{dm}{dt} & = & \frac{\partial m}{\partial\alpha}\frac{d\alpha}{dt}+\frac{\partial m}{\partial t}\\
 & = & \frac{\tau_{x}^{2}}{\tau_{\alpha}+\tau_{x}}+\frac{1}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that the difference is exactly what is expected from the above calculation
 -- the linear response variance adjustment is exact.
\end_layout

\begin_layout Section
Exact moments from MCMC
\end_layout

\begin_layout Standard
Note that we can use the law of total variance to avoid sampling variance
 for 
\begin_inset Formula $\lambda$
\end_inset

 standard deviations.
 Suppose we know exactly (e.g.
 because of conjugacy) 
\begin_inset Formula $\mbe\left[\theta\vert\alpha\right]$
\end_inset

 and 
\begin_inset Formula $\textrm{Var}\left(\theta\vert\alpha\right)$
\end_inset

.
 Then under 
\begin_inset Formula $\alpha$
\end_inset

 sampling
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(\theta\vert X\right) & = & \mbe\left[\textrm{Var}\left(\theta\vert\alpha,X\right)\vert X\right]+\textrm{Var}\left(\mbe\left[\theta\vert\alpha,X\right]\vert X\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...where the outer expectations can be done by Monte Carlo.
\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Part
Old writing, ignore past this point
\end_layout

\begin_layout Standard
Sanity check: the negative binomial distribution (using the Wikipedia, not
 the R parameterization) is
\begin_inset Formula 
\begin{eqnarray*}
\beta & = & \frac{1-\pi}{\pi}\\
\pi\beta+\pi & = & 1\\
\pi & = & \frac{1}{1+\beta}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(y_{i}\vert\gamma,\pi\right) & = & \frac{\Gamma\left(y_{i}+\gamma\right)}{\Gamma\left(\gamma\right)y!}\pi^{\gamma}\left(1-\pi\right)^{y_{i}}\\
\log p_{t}\left(y_{i}\vert\gamma,\pi\right) & = & \log\Gamma\left(y_{i}+\gamma\right)-\log\Gamma\left(\gamma\right)-\log y!+\gamma\log\left(1-\pi\right)+y_{i}\log\pi\\
 & = & \log\Gamma\left(y_{i}+\gamma\right)-\log\Gamma\left(\gamma\right)-\log y!+\gamma\log\left(\frac{\beta}{1+\beta}\right)+y_{i}\log\left(\frac{1}{1+\beta}\right)\\
 & = & \log\Gamma\left(y_{i}+\gamma\right)-\log\Gamma\left(\gamma\right)-\log y!-\gamma\log\left(1+\beta\right)+\gamma\log\left(\beta\right)-y_{i}\log\left(1+\beta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Re-fitting the moments
\end_layout

\begin_layout Standard
Let's take a classic EB problem, a simple Gamma-Poisson model.
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\vert\lambda_{i} & \sim & \textrm{Poisson}\left(\lambda_{i}\right)\\
\lambda_{i} & \sim & \textrm{Gamma}\left(\gamma,\beta\right)\\
\mbe\left[\lambda_{i}\right] & = & \frac{\gamma}{\beta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It is known that, marginally, 
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\vert\gamma,\beta & \sim & \textrm{NegBinom}\left(r,p\right)\\
\gamma & = & r\\
\beta & = & \frac{1-p}{p}\Rightarrow\\
p & = & \frac{1}{1+\beta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Typically, 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are chosen to maximize 
\begin_inset Formula $p\left(y\vert\gamma,\beta\right)$
\end_inset

 since 
\begin_inset Formula $\lambda_{i}$
\end_inset

 can be analytically marginalized out.
 However, if you knew 
\begin_inset Formula $\lambda_{i}$
\end_inset

, then 
\begin_inset Formula $y_{i}$
\end_inset

 becomes ancillary for 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

.
 In that case, a reasonable choice for 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 might be the MLE (or MAP with a prior):
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & -\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}+C\\
\hat{\gamma},\hat{\beta} & = & \textrm{argmax}\left\{ \frac{1}{N_{i}}\sum_{i}\left(-\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\right)\right\} \\
 & = & \textrm{argmax}\left\{ -\beta\frac{1}{N_{i}}\sum_{i}\lambda_{i}+\left(\gamma-1\right)\frac{1}{N}\sum_{i}\log\lambda_{i}\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is for a single sample, however, and we must choose a 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 to use for all the samples.
 A natural choice is the average MLE, where the average is taken over the
 posterior.
 In this case, 
\begin_inset Formula $\alpha=\left(\gamma,\beta\right)$
\end_inset

, 
\begin_inset Formula $\theta=\left(\lambda_{1},...,\lambda_{i},...,\lambda_{N_{i}}\right)$
\end_inset

 and
\begin_inset Formula 
\begin{eqnarray*}
\mu_{\alpha}\left(\lambda_{n},\gamma,\beta\right) & = & \left(\begin{array}{c}
\frac{1}{N_{i}}\sum_{i}\log\lambda_{in}\\
-\frac{1}{N_{i}}\sum_{i}\lambda_{in}
\end{array}\right)\\
\mu_{\alpha\alpha} & = & \left(\begin{array}{cc}
0 & 0\\
0 & 0
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Next,
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}+C\textrm{ (no dependence on }\alpha\textrm{)}\\
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\beta-\log\Gamma\left(\gamma\right)-\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log\rho\left(\lambda_{n},\gamma,\beta\right) & = & \log\prod_{i}p\left(y_{i}\vert\lambda_{i},\gamma,\beta\right)\\
 & = & \sum_{i}\log p\left(\lambda_{i}\vert\gamma,\beta\right)+C\\
 & = & N_{i}\gamma\log\beta-N_{i}\log\Gamma\left(\gamma\right)-\beta\sum_{i}\lambda_{i}+\left(\gamma-1\right)\sum_{i}\log\lambda_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\gamma}\log\rho\left(\lambda_{n},\gamma,\beta\right) & = & N_{i}\left(\log\beta-\psi\left(\gamma\right)\right)+\sum_{i}\log\lambda_{i}\\
 & = & N_{i}\left(\log\beta-\psi\left(\gamma\right)+\frac{1}{N_{i}}\sum_{i}\log\lambda_{i}\right)\\
\frac{\partial}{\partial\beta}\log\rho\left(\lambda_{n},\gamma,\beta\right) & = & N_{i}\frac{\gamma}{\beta}-\sum\lambda_{i}\\
 & = & N_{i}\left(\frac{\gamma}{\beta}-\frac{1}{N_{i}}\sum\lambda_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
These are the differences between the means as given by 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 and the sample means from the MCMC sample.
 Plugging in
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & \left[\frac{1}{n}\sum_{n}\frac{\partial}{\partial\alpha}\left.\log\rho\left(\alpha,\theta_{n}\right)\right|_{\hat{\alpha}}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)\right]^{-1}\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From this, we can see that
\begin_inset Formula 
\begin{eqnarray*}
\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)} & = & \widehat{\textrm{Cov}\left(\left(\begin{array}{c}
\lambda_{1n}\\
\vdots\\
\lambda_{in}\\
\vdots\\
\lambda_{N_{i}n}
\end{array}\right),\left(\begin{array}{c}
\frac{1}{N_{i}}\sum_{i}\log\lambda_{in}\\
-\frac{1}{N_{i}}\sum_{i}\lambda_{in}
\end{array}\right)\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
That is, the sample covariance between the individual draws and their own
 population means.
 And
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\alpha}\left.\log\rho\left(\alpha,\theta_{n}\right)\right|_{\hat{\alpha}}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right) & = & N_{i}\left(\begin{array}{c}
-\left(\psi\left(\gamma\right)-\log\beta\right)+\frac{1}{N_{i}}\sum_{i}\log\lambda_{i}\\
\frac{\gamma}{\beta}-\frac{1}{N_{i}}\sum\lambda_{i}
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{N_{i}}\sum_{i}\log\lambda_{in} & -\frac{1}{N_{i}}\sum_{i}\lambda_{in}\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is sensible -- the relationship between 
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

 is effected through 
\begin_inset Formula $\mu_{\alpha}$
\end_inset

, and we have the two covariances.
\end_layout

\begin_layout Section
Posterior moment calculations
\end_layout

\begin_layout Standard
Old writing
\end_layout

\begin_layout Standard
For the moment, assume that 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 is chosen as a function of some posterior moments, 
\begin_inset Formula $\mu$
\end_inset

, which are esimated with the draws 
\begin_inset Formula $\theta_{n}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha} & = & \underset{\alpha}{\textrm{argmax}}\left\{ \sum_{n}\mu\left(\theta_{n},\alpha\right)\right\} \\
0 & = & \sum_{n}\frac{\partial\mu\left(\theta_{n},\hat{\alpha}\right)}{\partial\alpha}:=\sum_{n}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It will be convenient to define
\begin_inset Formula 
\begin{eqnarray*}
\rho\left(\alpha,\theta\right) & := & p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)\\
\frac{\partial\rho\left(\alpha,\theta_{n}\right)}{\partial\alpha} & =: & \rho_{\alpha}\left(\alpha,\theta_{n}\right)\\
\frac{\partial\rho_{\alpha}\left(\alpha,\theta_{n}\right)}{\partial\alpha} & =: & \rho_{\alpha\alpha}\left(\alpha,\theta_{n}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $\alpha$
\end_inset

, the posterior of 
\begin_inset Formula $\theta$
\end_inset

 is given by
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta\vert x,\alpha\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\alpha\right)p\left(\theta\right)}{\int p\left(x\vert\theta,\alpha\right)p\left(\alpha\right)p\left(\theta\right)d\theta}\\
 & = & \frac{\rho\left(\theta,\alpha\right)}{\int\rho\left(\theta,\alpha\right)d\theta}\\
 & = & \frac{\rho\left(\theta,\alpha\right)}{c\left(\alpha\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Where it will also be covenient to define
\begin_inset Formula 
\begin{eqnarray*}
c\left(\alpha\right) & := & \int\rho\left(\theta,\alpha\right)d\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Next, suppose we had changed 
\begin_inset Formula $\hat{\alpha}$
\end_inset

 to 
\begin_inset Formula $\tilde{\alpha}$
\end_inset

.
 We can use importance sampling to estimate the change in the MCMC sample
 with weights given by
\begin_inset Formula 
\begin{eqnarray*}
w_{n} & = & \frac{p\left(\theta_{n}\vert\tilde{\alpha}\right)}{p\left(\theta_{n}\vert\hat{\alpha}\right)}\\
 & = & \frac{c\left(\hat{\alpha}\right)\rho\left(\tilde{\alpha},\theta_{n}\right)}{c\left(\tilde{\alpha}\right)\rho\left(\hat{\alpha},\theta_{n}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In order to estimate the covariance of 
\begin_inset Formula $\theta$
\end_inset

, we need to consider the tilted likelihood:
\begin_inset Formula 
\begin{eqnarray*}
p_{t}\left(\theta\vert x\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)\exp\left(t\theta\right)}{p_{t}\left(x\right)}\\
\rho_{t}\left(\theta,\alpha\right) & = & \rho\left(\theta,\alpha\right)\exp\left(t\theta\right)\\
c_{t}\left(\alpha\right) & = & \int\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)d\theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then conditional on the MCMC samples, which were drawn from 
\begin_inset Formula $p_{t}$
\end_inset

 with 
\begin_inset Formula $t=0$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & = & \underset{\alpha}{\textrm{argmax}}\left\{ \sum_{n}w_{t,n}\mu\left(\theta_{n},\alpha\right)\right\} \\
w_{t,n} & = & \frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha\right)}\cdot\frac{\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial t}\left.\frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha_{t}\right)}\right|_{t=0} & = & -\left.\frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha_{t}\right)^{2}}\int\theta\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)d\theta\right|_{t=0}\\
 & = & -\mbe_{\theta\vert x,\alpha}\left[\theta\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial t}\left.\frac{\rho\left(\theta,\alpha_{t}\right)\exp\left(t\theta\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\right|_{t=0} & = & \theta
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
To find the covariance of 
\begin_inset Formula $\theta$
\end_inset

, we need to determine
\begin_inset Formula 
\begin{eqnarray*}
m & := & \frac{1}{n}\sum_{n}\theta_{n}\\
m_{t} & := & \frac{1}{n}\sum_{n}w_{t,n}\theta_{n}\\
\hat{\Sigma}_{\theta} & = & \frac{dm_{t}}{dt}\\
 & = & \frac{1}{n}\sum_{n}\frac{dw_{t,n}}{dt}\theta_{n}\\
 & = & \frac{1}{n}\sum_{n}\left.\left(\frac{\partial w_{t,n}}{\partial\alpha}\frac{d\alpha}{dt}+\frac{\partial w_{t,n}}{\partial t}\right)\right|_{t=0}\theta_{n}
\end{eqnarray*}

\end_inset

Plugging in from above,
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial w_{t,n}}{\partial t}\right|_{t=0} & = & \left.\frac{c\left(\hat{\alpha}\right)}{c_{t}\left(\alpha\right)}\cdot\frac{\rho\left(\theta,\alpha\right)\exp\left(t\theta\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\right|_{t=0}\\
 & = & \left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\\
\left.\frac{\partial w_{t,n}}{d\alpha}\right|_{t=0} & = & \frac{\rho_{\alpha}\left(\alpha,\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\\
 & = & \left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \frac{1}{n}\sum_{n}\left.\left(\left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}\frac{d\alpha}{dt}+\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\right)\right|_{t=0}\theta_{n}\\
 & = & \frac{1}{n}\sum_{n}\left.\left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}\frac{d\alpha}{dt}\right|_{t=0}\theta_{n}+\frac{1}{n}\sum_{n}\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)^{T}\\
 & = & \frac{1}{n}\sum_{n}\left.\left.\frac{\partial}{\partial\alpha}\log\rho\left(\alpha,\theta_{n}\right)\right|_{\alpha=\hat{\alpha}}\frac{d\alpha}{dt}\right|_{t=0}\theta_{n}+\widehat{\textrm{Cov}\left(\theta\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that if 
\begin_inset Formula $\frac{d\alpha}{dt}=0$
\end_inset

 -- that is, if the estimate for 
\begin_inset Formula $\alpha$
\end_inset

 does not depend on the tilting of the likelihood -- then the estimated
 covariance of 
\begin_inset Formula $\theta$
\end_inset

 is simply the MCMC sample covariance.
\end_layout

\begin_layout Standard
Finally, bringing in the equations for 
\begin_inset Formula $\alpha_{t}$
\end_inset

, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
0 & = & \sum_{n}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Differnetiating both sides with respect to 
\begin_inset Formula $t$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
0 & = & \sum_{n}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\\
 & = & \sum_{n}\left(\frac{\partial}{\partial\alpha}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\frac{d\alpha}{dt}+\frac{\partial}{\partial t}w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\\
 & = & \sum_{n}\left(\left(\frac{\rho_{\alpha}\left(\hat{\alpha},\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)+w_{t,n}\left(\alpha_{t},\theta\right)\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\frac{d\alpha}{dt}+\left(\theta_{n}-\mbe_{\theta\vert x,\alpha}\left[\theta\right]\right)\mu_{\alpha}\left(\theta_{n},\alpha_{t}\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Scaling and evaluating at 
\begin_inset Formula $t=0$
\end_inset

 gives
\begin_inset Formula 
\begin{eqnarray*}
0 & = & \left[\frac{1}{n}\sum_{n}\left(\frac{\rho_{\alpha}\left(\hat{\alpha},\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)+\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\right]\frac{d\alpha}{dt}+\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus, solving for 
\begin_inset Formula $\frac{d\alpha}{dt}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & \left[\frac{1}{n}\sum_{n}\left(\frac{\rho_{\alpha}\left(\hat{\alpha},\theta_{n}\right)}{\rho\left(\hat{\alpha},\theta_{n}\right)}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)+\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\right]^{-1}\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
It is convenient to use the fact that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial\rho}{\partial\alpha} & = & \rho\frac{\partial\log\rho}{\partial\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\alpha}{dt} & = & \left[\frac{1}{n}\sum_{n}\left(\frac{\partial}{\partial\alpha}\left.\log\rho\left(\alpha,\theta_{n}\right)\right|_{\hat{\alpha}}\mu_{\alpha}\left(\theta_{n},\hat{\alpha}\right)+\mu_{\alpha\alpha}\left(\theta_{n},\alpha_{t}\right)\right)\right]^{-1}\widehat{\textrm{Cov}\left(\theta,\mu_{\alpha}\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Although cumbersome, this is a linear system only as big as the moment condition
s on 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Subsection
Exponential families
\end_layout

\begin_layout Standard
I think this is wrong.
\end_layout

\begin_layout Standard
If we have a conjugate exponential family:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(x_{i}\vert\theta_{i}\right) & = & \theta_{i}^{T}x_{i}-A_{x}\left(\theta_{i}\right)\\
\log p\left(\theta_{i}\vert\alpha\right) & = & \alpha_{\theta}^{T}\theta_{i}-\alpha_{x}A_{x}\left(\theta_{i}\right)-A\left(\alpha_{\theta},\alpha_{x}\right)\\
p\left(\theta_{i}\vert\alpha,t\right) & = & \frac{p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}}}{\int p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}}d\theta_{i}}\\
 & = & p\left(\theta_{i}\vert\alpha+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(x\vert\alpha,t\right) & = & \int\prod_{i}p\left(x_{i}\vert\theta_{i}\right)p\left(\theta_{i}\vert\alpha,t_{i}\right)d\theta\\
 & = & \frac{\int\prod_{i}p\left(x_{i}\vert\theta_{i}\right)p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}-t_{A,i}A_{x}\left(\theta_{i}\right)}p\left(\alpha\right)d\theta}{\int\prod_{i}p\left(\theta_{i}\vert\alpha\right)e^{\theta_{i}t_{i}+t_{A,i}A_{x}\left(\theta_{i}\right)}d\theta_{i}}\\
 & = & \frac{\int\exp\left(\sum_{i}\left(x_{i}+\alpha_{\theta}+t_{i}\right)^{T}\theta_{i}-\sum_{i}\left(1+\alpha_{x}+t_{A,i}\right)A_{x}\left(\theta_{i}\right)\right)d\theta}{\int\exp\left(\sum_{i}\left(\alpha_{\theta}+t_{i}\right)^{T}\theta_{i}-\sum_{i}\left(\alpha_{x}+t_{A,i}\right)A_{x}\left(\theta_{i}\right)\right)d\theta}\\
 & = & \exp\left(\sum_{i}\left(A\left(x_{i}+\alpha_{\theta}+t_{i},1+\alpha_{x}+t_{A,i}\right)-A\left(\alpha_{\theta}+t_{i},\alpha_{x}+t_{A,i}\right)\right)\right)\\
 & =: & \exp\left(\sum_{i}\left(A\left(x_{i}+\alpha+t_{i}\right)-A\left(\alpha+t_{i}\right)\right)\right)\textrm{ (slight abuse of notation)}
\end{eqnarray*}

\end_inset

In the case of the MLE for an exponential family,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
M & = & \sum_{i}\left(A\left(x_{i}+\alpha+t_{i}\right)-A\left(\alpha+t_{i}\right)\right)\\
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}} & = & \sum_{i}\left(\textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\right)-\textrm{Var}_{p\left(\theta_{i}\vert\alpha\right)}\left(\theta_{i}\right)\right)\\
\frac{\partial^{2}M}{\partial\alpha\partial t_{i}^{t}} & = & \textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\right)-\textrm{Var}_{p\left(\theta_{i}\vert\alpha\right)}\left(\theta_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The expectation is given by
\begin_inset Formula 
\begin{eqnarray*}
\log p_{t}\left(\theta_{i}\vert\alpha,x\right) & = & \left(\alpha_{\theta}+t_{i}\right)^{T}\theta_{i}-\left(\alpha_{x}+t_{A,i}\right)A_{x}\left(\theta_{i}\right)-A\left(\alpha_{\theta}+t_{i},\alpha_{x}+t_{A,i}\right)\Rightarrow\\
\frac{\partial\mbe_{p_{t}}\left[\theta_{i}\vert\alpha,x\right]}{\partial\alpha} & = & \textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\vert\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that since 
\begin_inset Formula $\frac{\partial^{2}M}{\partial\alpha\partial t_{i}^{t}}\ne\frac{\partial\mbe_{p_{t}}\left[\theta_{i}\vert\alpha,x\right]}{\partial\alpha}$
\end_inset

, the correction is not symmetric, nor is it guaranteed to be positive definite.
 Simulations also suggest that a better correction uses 
\begin_inset Formula $\frac{\partial^{2}M}{\partial\alpha\partial t_{i}^{t}}=\textrm{Var}_{p\left(\theta_{i}\vert x_{i}+\alpha\right)}\left(\theta_{i}\right)$
\end_inset

, which would also result in a symmetric, positive definite corrected matrix.
\end_layout

\begin_layout Standard

\series bold
\color red
Alp: can you find an error that would support this?
\end_layout

\end_body
\end_document
