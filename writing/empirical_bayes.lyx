#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbe}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Part
Standard Empirical Bayes
\end_layout

\begin_layout Section
Model and problem statement
\end_layout

\begin_layout Standard
Given a parameter 
\begin_inset Formula $\theta$
\end_inset

, a meta-parameter 
\begin_inset Formula $\alpha$
\end_inset

, and data 
\begin_inset Formula $x$
\end_inset

, we want the posterior:
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta,\alpha\vert x\right) & = & \frac{p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)d\theta d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We will do empirical Bayes on 
\begin_inset Formula $\alpha$
\end_inset

 and MCMC on 
\begin_inset Formula $\theta$
\end_inset

.
 Specifically, given
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha} & := & \underset{\alpha}{\textrm{argmax}}\left\{ M\left(\alpha\right)\right\} \\
M\left(\alpha\right) & := & \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\right)p\left(\alpha\right)d\theta\\
\theta_{n} & \sim & p\left(\theta\vert x,\hat{\alpha}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
A common question is how uncertainty in 
\begin_inset Formula $\alpha$
\end_inset

 would translate into uncertainty in 
\begin_inset Formula $\theta$
\end_inset

 if it were not fixed at 
\begin_inset Formula $\hat{\alpha}$
\end_inset

.
 We can try to answer this question with linear response.
 Recall that if we define the tilted posterior 
\begin_inset Formula $p\left(\theta,\alpha\vert x,t\right)$
\end_inset

 as
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta,\alpha\vert x,t\right) & \propto & p\left(\theta,\alpha\vert x\right)\exp\left(\theta^{T}t\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt^{T}}\left.\mbe\left[\theta\vert x,t\right]\right|_{t=0} & = & \textrm{Cov}\left(\theta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Under the empirical Bayes assumption that in a neighborhood of 
\begin_inset Formula $t=0$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta\vert x,t\right] & \approx & \mbe\left[\theta\vert x,\hat{\alpha}\left(t\right),t\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
it may be reasonable to also assume that
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Cov}\left(\theta\right)=\frac{d}{dt^{T}}\mbe\left[\theta\vert x,t\right] & \approx & \frac{d}{dt^{T}}\mbe\left[\theta\vert x,\hat{\alpha}\left(t\right),t\right]=:\hat{\Sigma}_{\theta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that we have written 
\begin_inset Formula $\hat{\alpha}\left(t\right)$
\end_inset

, since the optimal choice of 
\begin_inset Formula $\alpha$
\end_inset

 also depends on 
\begin_inset Formula $t$
\end_inset

.
 The right side can be evaluated explicitly without ever sampling from the
 full model, and this constitutes the linear response estimate of 
\begin_inset Formula $\textrm{Cov}\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Section
Evaluating 
\begin_inset Formula $\frac{d}{dt}\mbe\left[\theta\vert x,\hat{\alpha}\left(t\right),t\right]$
\end_inset

 
\end_layout

\begin_layout Standard
Let us write the tilted posterior explicitly:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(x,\theta,\alpha,t\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)e^{\theta t}d\theta d\alpha}\\
 & = & \frac{p\left(x\vert\theta\right)\frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'}\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'p\left(\alpha\right)}{\int p\left(x\vert\theta\right)p\frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'}\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'p\left(\alpha\right)d\theta d\alpha}\\
 & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'p\left(\alpha\right)}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)\int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'p\left(\alpha\right)d\theta d\alpha}\\
 & = & p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)\aleph\left(t\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Taking
\begin_inset Formula 
\begin{eqnarray*}
\aleph\left(\alpha,t\right) & := & \int p\left(\theta'\vert\alpha\right)e^{\theta't}d\theta'\\
p\left(\theta\vert\alpha,t\right) & = & \frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\aleph\left(\alpha,t\right)}\\
p\left(\alpha\vert t\right) & := & \frac{\aleph\left(\alpha,t\right)p\left(\alpha\right)}{\int\aleph\left(\alpha',t\right)p\left(\alpha'\right)d\alpha'}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Given this the tilted posterior is given by
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta,\alpha\vert x,t\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)d\theta d\alpha}\cdot\frac{\int\aleph\left(\alpha',t\right)p\left(\alpha'\right)d\alpha'}{\int\aleph\left(\alpha',t\right)p\left(\alpha'\right)d\alpha'}\\
 & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)}{\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)d\theta d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
\Sigma_{\theta}:=\frac{d}{dt^{T}}\left.\mbe_{p}\left[\theta\vert x,t\right]\right|_{t=0} & = & Cov_{p}\left(\theta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Under the assumption that the Empirical Bayes approximation is good, then
\begin_inset Formula 
\begin{eqnarray*}
\mbe_{p}\left[\theta\vert x,t\right] & \approx & \mbe_{p}\left[\theta\vert x,\alpha,t\right]\Rightarrow\\
\hat{\Sigma}_{\theta} & := & \frac{d}{dt^{T}}\mbe_{p}\left[\theta\vert x,\alpha,t\right]\approx\frac{d}{dt^{T}}\mbe_{p}\left[\theta\vert x,t\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\alpha$
\end_inset

 depends on 
\begin_inset Formula $t$
\end_inset

 since, as a function of 
\begin_inset Formula $t$
\end_inset

, we have
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & := & \underset{\alpha}{\textrm{argmax}}\left\{ \log p\left(\alpha\vert x,t\right)\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert t\right)p\left(\alpha\vert t\right)d\theta-\log\int\int p\left(x\vert\theta,\alpha'\right)p\left(\theta\vert t\right)p\left(\alpha'\vert t\right)d\theta d\alpha'\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert t\right)p\left(\alpha\vert t\right)d\theta\right\} \\
\hat{\alpha} & = & \hat{\alpha}_{0}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note at this point that 
\begin_inset Formula $\hat{\alpha}_{t}$
\end_inset

 is a function of 
\begin_inset Formula $t$
\end_inset

, and 
\begin_inset Formula $\mbe_{p}\left[\theta\vert x,\hat{\alpha}_{t},t\right]$
\end_inset

 is a function of both 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

.
 Taking the total derivative,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \left.\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha,t\right]}{\partial\alpha^{T}}\frac{d\alpha_{t}}{dt^{T}}\right|_{\alpha=\hat{\alpha}_{0},t=0}+\left.\frac{\partial\mbe_{p}\left[\theta\vert x,\hat{\alpha}_{0},t\right]}{\partial t}\right|_{t=0}\\
 & = & \left.\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha\right]}{\partial\alpha^{T}}\frac{d\alpha_{t}}{dt^{T}}\right|_{\alpha=\hat{\alpha}_{0},t=0}+\textrm{Cov}\left(\theta\vert x,\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In order to evaluate 
\begin_inset Formula $\frac{d}{dt^{T}}\mbe_{p}\left[\theta\vert x,\alpha_{t},t\right]$
\end_inset

 we will need two terms: 
\begin_inset Formula $\frac{\partial}{\partial\alpha^{T}}\mbe_{p}\left[\theta\vert x,\alpha\right]$
\end_inset

 and 
\begin_inset Formula $\frac{d\alpha_{t}}{dt}$
\end_inset

.
 Note that 
\begin_inset Formula $\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha,t\right]}{\partial t}=\textrm{Cov}\left(\theta\vert x,\alpha\right)$
\end_inset

, so this has the form of the conditional variance plus a correction term.
\end_layout

\begin_layout Subsection
\begin_inset Formula $d\alpha/dt$
\end_inset


\end_layout

\begin_layout Standard
First, we will calculate 
\begin_inset Formula $d\alpha/dt$
\end_inset

.
 Recall that
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha}_{t} & := & \underset{\alpha}{\textrm{argmax}}\left\{ p\left(\alpha\vert x,t\right)\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \frac{p\left(x\vert\alpha,t\right)p\left(\alpha\vert t\right)}{\int p\left(x\vert\alpha',t\right)p\left(\alpha'\vert t\right)d\alpha'}\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \frac{\int p\left(x\vert\alpha\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)d\theta}{\int\int p\left(x\vert\alpha'\right)p\left(\theta'\vert\alpha',t\right)p\left(\alpha'\vert t\right)d\alpha'd\theta'}\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)d\theta-\log\int\int p\left(x\vert\theta,\alpha'\right)p\left(\theta\vert\alpha',t\right)p\left(\alpha'\vert t\right)d\theta d\alpha'\right\} \\
 & = & \underset{\alpha}{\textrm{argmax}}\left\{ \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)d\theta\right\} \textrm{ (the constant doesn't depend on }\alpha\textrm{)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Let us define 
\begin_inset Formula 
\begin{eqnarray*}
M\left(\alpha,t\right) & := & \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)p\left(\alpha\vert t\right)d\theta\\
 & = & \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)d\theta+\log p\left(\alpha\vert t\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Expanding in terms of the original distributions,
\begin_inset Formula 
\begin{eqnarray*}
\log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha,t\right)d\theta+\log p\left(\alpha\vert t\right) & = & \log\int p\left(x\vert\theta,\alpha\right)\frac{p\left(\theta\vert\alpha\right)e^{\theta t}}{\aleph\left(\alpha,t\right)}d\theta+\log\frac{\aleph\left(\alpha,t\right)p\left(\alpha\right)}{\int\aleph\left(\alpha',t\right)d\alpha'}\\
 & = & \log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)e^{\theta t}d\theta+\log p\left(\alpha\right)-\log\int\aleph\left(\alpha',t\right)d\alpha
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Interestingly, note that if you perform the MLE version of EB, then the
 term 
\begin_inset Formula $\aleph\left(\alpha,t\right)$
\end_inset

 does not cancel and the result is not necessarily symmetric.
 The last term does not depends only on 
\begin_inset Formula $t$
\end_inset

, and so does not affect the optimization (partials with respect to 
\begin_inset Formula $\alpha$
\end_inset

 are zero).
 So
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right|_{t=0} & = & \frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\left(\log\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)d\theta+\log p\left(\alpha\right)\right)\\
\frac{\partial}{\partial\alpha}\left.\frac{\partial M}{\partial t^{T}}\right|_{t=0} & = & \frac{\partial}{\partial\alpha}\left(\frac{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)\theta^{T}d\theta}{\int p\left(x\vert\theta,\alpha\right)p\left(\theta\vert\alpha\right)d\theta}\right)\\
 & = & \frac{\partial}{\partial\alpha}\mbe\left[\theta\vert x,\alpha\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For any 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\hat{\alpha}_{t}$
\end_inset

 is chosen so that
\begin_inset Formula 
\begin{eqnarray*}
\left.\frac{\partial M}{\partial\alpha}\right|_{t,\hat{\alpha}_{t}} & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
we can differentiate using the chain rule, giving
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\frac{d\alpha_{t}}{dt}+\frac{\partial^{2}M}{\partial\alpha\partial t} & = & 0\Rightarrow\\
\frac{d\alpha}{dt} & = & -\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\frac{\partial^{2}M}{\partial\alpha\partial t^{T}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that since 
\begin_inset Formula $M$
\end_inset

 is maximized, its Hessian will be negative definite at 
\begin_inset Formula $\hat{\alpha}_{t}$
\end_inset

.
 
\end_layout

\begin_layout Subsection
\begin_inset Formula $\partial\mbe_{p}\left[\theta\vert x,\alpha\right]/\partial\alpha$
\end_inset


\end_layout

\begin_layout Standard
In some cases, this derivative can be computed exactly (both of our examples
 below have this property).
 In general, it can be evaluated with MCMC draws from the EB model by differenti
ating under the integral:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\alpha}\mbe_{p}\left[\theta\vert x,\alpha\right] & = & \frac{\partial}{\partial\alpha}\int\theta p\left(\theta\vert x,\alpha,t\right)d\theta\\
 & = & \int\theta\frac{\partial}{\partial\alpha}\log p\left(\theta\vert x,\alpha,t\right)p\left(\theta\vert x,\alpha,t\right)d\theta\\
 & = & \int\theta\left(\frac{\partial}{\partial\alpha}\log p\left(\theta\vert x,\alpha,t\right)-\mbe\left[\frac{\partial}{\partial\alpha}\log p\left(\theta\vert x,\alpha,t\right)\vert x,\alpha,t\right]\right)p\left(\theta\vert x,\alpha,t\right)d\theta\\
 & = & \int\left(\theta-\mbe\left[\theta\vert x,\alpha,t\right]\right)\left(\frac{\partial}{\partial\alpha}\log p\left(\theta\vert x,\alpha,t\right)-\mbe\left[\frac{\partial}{\partial\alpha}\log p\left(\theta\vert x,\alpha,t\right)\vert x,\alpha,t\right]\right)p\left(\theta\vert x,\alpha,t\right)d\theta\\
 & = & \textrm{Cov}\left(\theta,\frac{\partial}{\partial\alpha}\log p\left(\theta\vert x,\alpha,t\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where we have used the fact that
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\frac{\partial}{\partial\alpha}\log p\left(\theta\vert x,\alpha,t\right)\vert x,\alpha,t\right] & = & 0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This covariance can be estimated with MCMC draws.
\end_layout

\begin_layout Subsection
Putting it all together
\end_layout

\begin_layout Standard
Putting together the above results and using the fact that 
\begin_inset Formula $\frac{\partial^{2}M}{\partial\alpha^{T}\partial t}=\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha\right]}{\partial\alpha^{T}}$
\end_inset

, we see that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\hat{\Sigma}_{\theta} & = & \left.\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha\right]}{\partial\alpha^{T}}\frac{d\alpha_{t}}{dt^{T}}\right|_{\alpha=\hat{\alpha}_{0},t=0}+\textrm{Cov}\left(\theta\vert x,\alpha\right)\\
 & = & -\left.\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha\right]}{\partial\alpha^{T}}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\frac{\partial\mbe_{p}\left[\theta\vert x,\alpha\right]}{\partial\alpha}\right)^{T}\right|_{\hat{\alpha}_{0}}+\textrm{Cov}\left(\theta\vert x,\hat{\alpha}_{0}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $M$
\end_inset

 is maximized at 
\begin_inset Formula $\hat{\alpha}_{0}$
\end_inset

, this has the form of a positive definite correction to the conditional
 covariance matrix in terms that can be evaluated only at the EB solution.
 Note that it is exactly the naive form that you would make with a Laplace
 approximation to 
\begin_inset Formula $p\left(\alpha\vert x\right)$
\end_inset

 at 
\begin_inset Formula $\hat{\alpha}_{0}$
\end_inset

 and a linear assumption on 
\begin_inset Formula $\mbe_{p}\left[\theta\vert x,\alpha\right]$
\end_inset

, though the assumptions seem superficially to have quite a different form.
 It is interesting to ask whether the key assumption
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt^{T}}\mbe\left[\theta\vert x,t\right] & \approx & \frac{d}{dt^{T}}\mbe\left[\theta\vert x,\hat{\alpha}\left(t\right),t\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
has made an implicit normal approximation somehow.
\end_layout

\begin_layout Section
Gamma Poisson Example
\end_layout

\begin_layout Standard
Let's take a classic EB problem, a simple Gamma-Poisson model.
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\vert\lambda_{i} & \sim & \textrm{Poisson}\left(\lambda_{i}\right)\textrm{ for }i=1,...,N\\
\lambda_{i} & \sim & \textrm{Gamma}\left(\gamma,\beta\right)\\
\mbe\left[\lambda_{i}\right] & = & \frac{\gamma}{\beta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Additionally, we may put gamma priors on 
\begin_inset Formula $\gamma$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 (at the risk of having too many gammas around):
\begin_inset Formula 
\begin{eqnarray*}
\gamma & \sim & \textrm{Gamma}\left(a_{\gamma},b_{\gamma}\right)\\
\beta & \sim & \textrm{Gamma}\left(a_{\beta},b_{\beta}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In the above notation, we have
\begin_inset Formula 
\begin{eqnarray*}
x & = & y\\
\theta & = & \left(\lambda_{1},...,\lambda_{N}\right)^{T}\\
\alpha & = & \left(\gamma,\beta\right)^{T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Variance of 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard
Note that it is a standard result that the EB objective can be marginalized
 exactly :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}-\log y_{i}!\\
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\beta-\log\Gamma\left(\gamma\right)-\beta\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log p\left(y,\lambda\vert\gamma,\beta\right) & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\beta+1\right)\lambda_{i}+\sum_{i}\left(\gamma+y_{i}-1\right)\log\lambda_{i}-\sum_{i}\log y_{i}!\\
 & = & N\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\log y_{i}!\\
 &  & +\sum_{i}\left[-\left(\beta+1\right)\lambda_{i}+\left(\gamma+y_{i}-1\right)\log\lambda_{i}+\left(\gamma+y_{i}\right)\log\left(\beta+1\right)-\log\Gamma\left(\gamma+y_{i}\right)\right]\\
 &  & -\sum_{i}\left[\left(\gamma+y_{i}\right)\log\left(\beta+1\right)-\log\Gamma\left(\gamma+y_{i}\right)\right]\Rightarrow\\
\log p\left(y\vert\gamma,\beta\right) & = & \sum_{i}\gamma\log\beta-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
Hessian of 
\begin_inset Formula $M$
\end_inset


\end_layout

\begin_layout Standard
First, we need to find the Hessian of the marginal posterior at the optimum:
\begin_inset Formula 
\begin{eqnarray*}
M & = & \log p\left(y\vert\gamma,\beta\right)+\log p\left(\gamma\right)+\log p\left(\beta\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From above, we have
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y\vert\gamma,\beta\right) & = & N\gamma\log\left(\beta\right)-N\log\Gamma\left(\gamma\right)-\sum_{i}\left(\gamma+y_{i}\right)\log\left(\beta+1\right)+\sum_{i}\log\Gamma\left(\gamma+y_{i}\right)-\sum_{i}\log y_{i}!\\
\frac{\partial}{\partial\gamma}\log p\left(y\vert\gamma,\beta\right) & = & \sum_{i}\log\left(\beta\right)-N\psi\left(\gamma\right)-\sum_{i}\log\left(\beta+1\right)+\sum_{i}\psi\left(\gamma+y_{i}\right)\\
\frac{\partial}{\partial\beta}\log p\left(y\vert\gamma,\beta\right) & = & \sum_{i}\left(\frac{\gamma}{\beta}-\frac{\gamma+y_{i}}{1+\beta}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}\log p\left(y\vert\gamma,\beta\right)}{\partial\alpha\partial\alpha^{T}} & = & N\left(\begin{array}{cc}
\frac{1}{N}\sum_{i}\left(\psi'\left(\gamma+y_{i}\right)-\psi'\left(\gamma\right)\right) & \frac{1}{\beta}-\frac{1}{1+\beta}\\
\frac{1}{\beta}-\frac{1}{1+\beta} & \frac{1}{N}\sum_{i}\left(\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}-\frac{\gamma}{\beta^{2}}\right)
\end{array}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Recall that here 
\begin_inset Formula $N$
\end_inset

 refers to observations within a sample, not the number of MCMC draws.
 The prior's contribution to the Hessian is given by
\begin_inset Formula 
\begin{eqnarray*}
\gamma & \sim & \textrm{Gamma}\left(a_{\gamma},b_{\gamma}\right)\\
\log p\left(\gamma\right) & = & -b_{\gamma}\gamma+\log\left(\gamma\right)\left(a_{\gamma}-1\right)+C\\
\frac{\partial^{2}\log p\left(\gamma\right)}{\partial\gamma^{2}} & = & -\frac{a_{\gamma}-1}{\gamma^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
And similarly
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}\log p\left(\beta\right)}{\partial\beta^{2}} & = & -\frac{a_{\beta}-1}{\beta^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Putting this together,
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}} & = & N\left(\begin{array}{cc}
\frac{1}{N}\sum_{i}\left(\psi'\left(\gamma+y_{i}\right)-\psi'\left(\gamma\right)\right) & \frac{1}{\beta}-\frac{1}{1+\beta}\\
\frac{1}{\beta}-\frac{1}{1+\beta} & \frac{1}{N}\sum_{i}\left(\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}-\frac{\gamma}{\beta^{2}}\right)
\end{array}\right)-\frac{a_{\gamma}-1}{\gamma^{2}}-\frac{a_{\beta}-1}{\beta^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Exact moments
\end_layout

\begin_layout Standard
Instead of using MCMC, in this case we can use exact moments.
 Specifically,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(y_{i}\vert\lambda_{i}\right) & = & -\lambda_{i}+y_{i}\log\lambda_{i}-\log y_{i}!\\
\log p\left(\lambda_{i}\vert\gamma,\beta\right) & = & \gamma\log\left(\beta\right)-\log\Gamma\left(\gamma\right)-\left(\beta\right)\lambda_{i}+\left(\gamma-1\right)\log\lambda_{i}\\
\log p\left(\lambda_{i}\vert y_{i},\gamma,\beta\right) & = & \left(y_{i}+\gamma\right)\log\left(\beta+1\right)-\log\Gamma\left(y_{i}+\gamma\right)-\left(\beta+1\right)\lambda_{i}+\left(y_{i}+\gamma-1\right)\log\lambda_{i}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\mbe_{p_{t}}\left[\lambda_{i}\vert\alpha,y_{i}\right] & = & \frac{\gamma+y_{i}}{\beta+1}:=m_{i}\left(\alpha\right)\\
\left.\frac{\partial m_{i}}{\partial\gamma}\right|_{t_{i}=0} & = & \frac{1}{1+\beta}\\
\left.\frac{\partial m_{i}}{\partial\beta}\right|_{t_{i}=0} & = & -\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}_{t}\left(\lambda_{i}\right) & = & -\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)^{T}\left(\frac{\partial^{2}M}{\partial\alpha\partial\alpha^{T}}\right)^{-1}\left(\begin{array}{c}
\frac{1}{1+\beta}\\
-\frac{\gamma+y_{i}}{\left(1+\beta\right)^{2}}
\end{array}\right)+\frac{y_{i}+\gamma}{\left(\beta+1\right)^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Gamma moments
\end_layout

\begin_layout Standard
For reference, here are some moments of a gamma distribution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
x & \sim & \textrm{Gamma}\left(\alpha,\beta\right)\\
p\left(x\right) & = & \frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}x^{\alpha-1}e^{-\beta x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[x^{k}\right] & = & \int\frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}x^{\alpha-1}e^{-\beta x}x^{k}dx\\
 & = & \frac{\beta^{\alpha}}{\Gamma\left(\alpha\right)}\frac{\Gamma\left(k+\alpha\right)}{\beta^{k+\alpha}}\int\frac{\beta^{k+\alpha}}{\Gamma\left(k+\alpha\right)}x^{k+\alpha-1}e^{-\beta x}dx\\
 & = & \frac{\Gamma\left(k+\alpha\right)}{\Gamma\left(\alpha\right)}\beta^{-k}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The variance of the sample variance is given by
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(s^{2}\right) & = & \frac{\left(N-1\right)^{2}}{N^{3}}\mu_{4}-\frac{\left(N-1\right)\left(N-3\right)\mu_{2}^{2}}{N^{3}}\\
\mu_{4} & = & \mbe\left[\left(x-\mu\right)^{4}\right]\\
 & = & \mbe\left[x^{4}-4x^{3}\mu+6x^{2}\mu^{2}-4x\mu^{3}+\mu^{4}\right]\\
 & = & m_{4}-4m_{3}\mu+6m_{2}\mu^{2}-4\mu^{4}+\mu^{4}\\
 & = & m_{4}-4m_{3}\mu+6m_{2}\mu^{2}-3\mu^{4}\\
\mu_{2} & = & m_{2}-\mu^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then by the delta method
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(\sqrt{x}\right) & \approx & \frac{\textrm{Var}\left(x\right)}{4\mbe\left[x\right]}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Rao-Blackwellization of variance estimates
\end_layout

\begin_layout Standard
Note that we can use the law of total variance to avoid sampling variance
 for 
\begin_inset Formula $\lambda$
\end_inset

 standard deviations.
 Suppose we know exactly (e.g.
 because of conjugacy) 
\begin_inset Formula $\mbe\left[\theta\vert\alpha\right]$
\end_inset

 and 
\begin_inset Formula $\textrm{Var}\left(\theta\vert\alpha\right)$
\end_inset

.
 Then under 
\begin_inset Formula $\alpha$
\end_inset

 sampling
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Var}\left(\theta\vert X\right) & = & \mbe\left[\textrm{Var}\left(\theta\vert\alpha,X\right)\vert X\right]+\textrm{Var}\left(\mbe\left[\theta\vert\alpha,X\right]\vert X\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...where the outer expectations can be done by Monte Carlo.
 This can reduce the number of MCMC samples necessary to accurately estimate
 variances.
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Standard
Here I show some simple results.
 The 
\begin_inset Quotes eld
\end_inset

free model
\begin_inset Quotes erd
\end_inset

 refers to a full MCMC model marginalized over 
\begin_inset Formula $\alpha$
\end_inset

, where the 
\begin_inset Quotes eld
\end_inset

fixed model
\begin_inset Quotes erd
\end_inset

 refers to the model where 
\begin_inset Formula $\alpha$
\end_inset

 is fixed at its EB estimate, 
\begin_inset Formula $\hat{\alpha}_{0}$
\end_inset

.
 Here I have chosen a set of parameters where the linear response works
 well, as shown in the third plot.
 However, when the posterior on 
\begin_inset Formula $\alpha$
\end_inset

 is less concentrated it does not work very well.
 
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

echo=FALSE,warning=FALSE,message=FALSE
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

library(ggplot2) 
\end_layout

\begin_layout Plain Layout

library(dplyr) 
\end_layout

\begin_layout Plain Layout

library(reshape2) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

project_directory <- file.path(Sys.getenv("GIT_REPO_LOC"), "EmpiricalBayes/gamma_p
oisson")
\end_layout

\begin_layout Plain Layout

data_directory <- file.path(project_directory, "data/")
\end_layout

\begin_layout Plain Layout

load(file.path(data_directory, "simulation_example.Rdata"))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

echo=FALSE,warning=FALSE,message=FALSE,fig.width=4,fig.height=3
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

ggplot(lambda_true_df) +
\end_layout

\begin_layout Plain Layout

  geom_point(aes(x=free_mean, y=fixed_mean)) +
\end_layout

\begin_layout Plain Layout

  geom_abline(aes(slope=1, intercept=0)) +
\end_layout

\begin_layout Plain Layout

  xlab("MCMC mean for the free model") +
\end_layout

\begin_layout Plain Layout

  ylab("MCMC mean for the fixed model")
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

echo=FALSE,warning=FALSE,message=FALSE,fig.width=4,fig.height=3
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

ggplot(lambda_true_df) +
\end_layout

\begin_layout Plain Layout

  geom_point(aes(x=free_var, y=fixed_var)) +
\end_layout

\begin_layout Plain Layout

  geom_abline(aes(slope=1, intercept=0)) +
\end_layout

\begin_layout Plain Layout

  xlab("MCMC variance for the free model") +
\end_layout

\begin_layout Plain Layout

  ylab("MCMC variancefor the fixed model")
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout

echo=FALSE,warning=FALSE,message=FALSE,fig.width=4,fig.height=3
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

ggplot(lambda_stats_correction) +   
\end_layout

\begin_layout Plain Layout

  geom_point(aes(x=mcmc_diff, y=lr_diff_exact)) +   
\end_layout

\begin_layout Plain Layout

  geom_abline(aes(slope=1, intercept=0)) +   
\end_layout

\begin_layout Plain Layout

  expand_limits(x=0, y=0) +   
\end_layout

\begin_layout Plain Layout

  xlab("Difference between MCMC runs") +   
\end_layout

\begin_layout Plain Layout

  ylab("Difference predicted by linear response") +   
\end_layout

\begin_layout Plain Layout

  ggtitle("Difference in standard deviation
\backslash
nbetween free and fixed priors")
\end_layout

\end_inset


\end_layout

\begin_layout Section
Normal-Normal Example
\end_layout

\begin_layout Standard
Here, I show that the correction is exact for Normal mixtures of Normals.
 This is not surprising, but was helpful in tidying up the above results.
\end_layout

\begin_layout Standard
Let's take an even more classic example:
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\theta_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,\tau^{-1}\right)\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\tau,\tau_{\alpha} &  & \textrm{are known}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(x_{i}\vert\theta_{i}\right) & = & -\frac{1}{2}\left(x_{i}-\theta_{i}\right)^{2}+C\\
 & = & -\frac{1}{2}x_{i}^{2}+x_{i}\theta_{i}-\frac{1}{2}\theta_{i}^{2}\\
\log p\left(\theta_{i}\vert\alpha\right) & = & -\frac{1}{2}\left(\tau\left(\theta_{i}-\alpha\right)^{2}-\log\tau\right)+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha-\frac{1}{2}\tau\alpha^{2}-\frac{1}{2}\log\tau+C\\
\log p\left(\alpha\right) & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
True posterior: integrating out 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
Here, we know that marginally over 
\begin_inset Formula $\alpha$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i} & \sim & \mathcal{N}\left(0,\tau_{\theta}^{-1}\right)\\
\tau_{\theta} & := & \left(\tau^{-1}+\tau_{\alpha}^{-1}\right)^{-1}\\
 & = & \frac{\tau\tau_{\alpha}}{\tau+\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that
\begin_inset Formula 
\begin{eqnarray*}
\tau_{\theta}^{-1} & = & \tau^{-1}+\tau_{\alpha}^{-1}>\tau^{-1}\Rightarrow\\
\tau & > & \tau_{\theta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus the posterior is given by
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert x_{i}\right] & = & \frac{1}{\tau_{\theta}+1}\left(0\cdot\tau_{\theta}+x_{i}\right)\\
 & = & \frac{x_{i}}{\tau_{\theta}+1}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert x_{i} & \sim & \mathcal{N}\left(\frac{1}{\tau_{\theta}+1}\cdot x_{i},\left(\tau_{\theta}+1\right)^{-1}\right)\\
\frac{1}{\tau_{\theta}+1} & = & \frac{\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau+\tau\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is the variance we hope to recover with linear response.
 
\end_layout

\begin_layout Subsection
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset

 for Empirical Bayes
\end_layout

\begin_layout Standard
Integrating out 
\begin_inset Formula $\theta_{i}$
\end_inset

,
\begin_inset Formula 
\begin{eqnarray*}
x_{i}\vert\alpha & \sim & \mathcal{N}\left(\alpha,\tau_{x}^{-1}\right)\\
\tau_{x} & := & \left(1+\tau^{-1}\right)^{-1}=\frac{\tau}{1+\tau}\\
\alpha & \sim & \mathcal{N}\left(0,\tau_{\alpha}^{-1}\right)\\
\mbe\left[\alpha\vert x_{i}\right] & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that this is maximized at 
\begin_inset Formula $\hat{\alpha}=\frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}x_{i}$
\end_inset

.
 Thus the EB posterior of 
\begin_inset Formula $\theta_{i}$
\end_inset

 is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\hat{\alpha} & \sim & \mathcal{N}\left(\frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}x_{i},\tau^{-1}\right)\\
x_{i}\vert\theta_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\textrm{Var}\left(\theta_{i}\vert\hat{\alpha},x_{i}\right) & = & \left(1+\tau\right)^{-1}\\
\mbe\left[\theta_{i}\vert\hat{\alpha},x_{i}\right] & = & \frac{1}{1+\tau}\left(\frac{\tau_{x}\tau}{\tau_{\alpha}+\tau_{x}}+1\right)x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Dig that
\begin_inset Formula 
\begin{eqnarray*}
\frac{\tau_{x}\tau}{\tau_{\alpha}+\tau_{x}} & = & \frac{\tau^{2}}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
\frac{\tau_{x}\tau}{\tau_{\alpha}+\tau_{x}}+1 & = & \frac{\tau^{2}+\tau_{\alpha}+\tau_{\alpha}\tau+\tau}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
 & = & \frac{\tau\left(\tau+\tau_{\alpha}\right)+\tau_{\alpha}+\tau}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
 & = & \frac{\left(1+\tau\right)\left(\tau+\tau_{\alpha}\right)}{\tau_{\alpha}+\tau_{\alpha}\tau+\tau}\\
 & = & \frac{1+\tau}{1+\tau_{\theta}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
\mbe\left[\theta_{i}\vert\hat{\alpha},x_{i}\right] & = & \frac{1}{1+\tau_{\theta}}x_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Here, in contrast to the correct model with 
\begin_inset Formula $\alpha$
\end_inset

 marginalized out, 
\begin_inset Formula $\theta_{i}\vert\hat{\alpha},x_{i}$
\end_inset

 has an estimated variance that is too small since 
\begin_inset Formula $\tau>\tau_{m}$
\end_inset

.
 However, the shrinkage matches the marginal case exactly.
 The difference in the variances is given by
\begin_inset Formula 
\begin{eqnarray*}
\frac{1}{1+\tau_{\theta}}-\frac{1}{1+\tau} & = & \frac{\tau+\tau_{\alpha}}{\tau+\tau\tau_{\alpha}+\tau_{\alpha}}-\frac{1}{1+\tau}\\
 & = & \frac{\left(1+\tau\right)\left(\tau+\tau_{\alpha}\right)-\tau-\tau\tau_{\alpha}-\tau_{\alpha}}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)\left(1+\tau\right)}\\
 & = & \frac{\tau+\tau_{\alpha}+\tau^{2}+\tau\tau_{\alpha}-\tau-\tau\tau_{\alpha}-\tau_{\alpha}}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)\left(1+\tau\right)}\\
 & = & \frac{\tau^{2}}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)\left(1+\tau\right)}\\
 & = & \frac{\tau_{x}^{2}\left(1+\tau\right)}{\left(\tau+\tau\tau_{\alpha}+\tau_{\alpha}\right)}\\
 & = & \frac{\tau_{x}^{2}\left(1+\tau\right)}{\left(\tau+\left(\tau+1\right)\tau_{\alpha}\right)}\\
 & = & \frac{\tau_{x}^{2}}{\frac{\tau}{\left(1+\tau\right)}+\tau_{\alpha}}\\
 & = & \frac{\tau_{x}^{2}}{\tau_{x}+\tau_{\alpha}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Tilted model
\end_layout

\begin_layout Standard
Suppose we are interested in the variance of 
\begin_inset Formula $\theta_{i}$
\end_inset

.
 Then we perturb with 
\begin_inset Formula $t_{i}\theta_{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(x,\theta,\alpha,t\right) & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)}{\int\int\int p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)dxd\theta d\alpha}\\
 & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)}{\int\int p\left(\theta\vert\alpha\right)\exp\left(t\theta\right)p\left(\alpha\right)d\theta d\alpha}\\
 & = & \frac{p\left(x\vert\theta\right)p\left(\theta\vert\alpha\right)p\left(\alpha\right)\exp\left(t\theta\right)}{\int\int p\left(\theta\vert\alpha\right)\exp\left(t\theta\right)d\theta p\left(\alpha\right)d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(\theta_{i},\alpha\vert t_{i}\right) & = & \frac{p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)}{\int\int p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
p\left(\alpha\vert t_{i}\right) & = & \frac{\int p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}}{\int\int p\left(\theta_{i}\vert\alpha,t_{i}\right)p\left(\alpha\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}d\alpha}\\
 & = & \frac{p\left(\alpha\right)\int p\left(\theta_{i}\vert\alpha,t_{i}\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}}{\int p\left(\alpha\right)\left[\int p\left(\theta_{i}\vert\alpha,t_{i}\right)\exp\left(t_{i}\theta_{i}\right)d\theta_{i}\right]d\alpha}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\log p\left(\theta_{i}\vert\alpha,t_{i}\right) & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\theta_{i}\alpha-\frac{1}{2}\tau\alpha^{2}+t_{i}\theta_{i}+C\\
 & = & -\frac{1}{2}\tau\theta_{i}^{2}+\tau\left(\alpha+\tau^{-1}t_{i}\right)\theta_{i}-\frac{1}{2}\tau\left(\alpha+\tau^{-1}t_{i}\right)^{2}+\frac{1}{2}\tau\left(\alpha+\tau^{-1}t_{i}\right)^{2}-\frac{1}{2}\tau\alpha^{2}+C\\
 & = & -\frac{1}{2}\tau\left(\theta_{i}-\left(\alpha+\tau^{-1}t_{i}\right)\right)^{2}+\frac{1}{2}\tau\left(\alpha+\tau^{-1}t_{i}\right)^{2}-\frac{1}{2}\tau\alpha^{2}+C\\
 & = & \log p\left(\theta_{i}\vert\alpha_{t},t_{i}\right)+\frac{1}{2}\tau\left(\alpha^{2}+2\tau^{-1}t_{i}\alpha+\tau^{-2}t_{i}^{2}\right)-\frac{1}{2}\tau\alpha^{2}+C\\
 & = & \log p\left(\theta_{i}\vert\alpha_{t},t_{i}\right)+t_{i}\alpha+C\\
\alpha_{t} & := & \alpha+\tau^{-1}t_{i}\\
\log p\left(\alpha\vert t_{i}\right) & = & \log p\left(\alpha\right)++t_{i}\alpha+C\\
 & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}+t_{i}\alpha+C\\
 & = & -\frac{1}{2}\tau_{\alpha}\alpha^{2}+\tau_{\alpha}\tau_{\alpha}^{-1}t_{i}\alpha+C\\
 & = & -\frac{1}{2}\tau_{\alpha}\left(\alpha-\tau_{\alpha}^{-1}t_{i}\right)^{2}+C
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\alpha\vert t_{i} & \sim & \mathcal{N}\left(\tau_{\alpha}^{-1}t_{i},\tau_{\alpha}^{-1}\right)\\
\theta_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau^{-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that, as expected,
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt_{i}}\mbe\left[\theta_{i}\vert\alpha,t_{i}\right] & = & \tau^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note also that
\begin_inset Formula 
\begin{eqnarray*}
\textrm{Cov}\left(\theta_{i},\alpha\right) & = & \mbe\left[\theta_{i}\alpha\right]-\mbe\left[\theta_{i}\right]\mbe\left[\alpha\right]\\
 & = & \mbe\left[\alpha\mbe\left[\theta_{i}\vert\alpha\right]\right]-\mbe\left[\alpha^{2}\right]\\
 & = & \mbe\left[\alpha^{2}\right]-\mbe\left[\alpha^{2}\right]\\
 & = & \textrm{Var}\left(\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
and that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt_{i}}\mbe\left[\alpha\vert t_{i}\right] & = & \tau_{\alpha}^{-1}\\
 & = & \textrm{Var}\left(\alpha\right)\\
 & = & \textrm{Cov}\left(\theta_{i},\alpha\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
...also as expected.
\end_layout

\begin_layout Subsubsection
Exact tilted posterior
\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $t_{i}$
\end_inset

 we can again integrate out 
\begin_inset Formula $\alpha$
\end_inset

.
 First observe that
\begin_inset Formula 
\begin{eqnarray*}
\tau_{\alpha}^{-1}+\tau^{-1} & = & \frac{1}{\tau_{\alpha}}+\frac{1}{\tau}\\
 & = & \frac{\tau+\tau_{\alpha}}{\tau_{\alpha}\tau}\\
 & = & \tau_{\theta}^{-1}
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert t_{i} & \sim & \mathcal{N}\left(\tau_{\theta}^{-1}t_{i},\tau_{\theta}^{-1}\right)\\
x_{i}\vert\theta_{i},t_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\Rightarrow\\
\mbe\left[\theta_{i}\vert x_{i},t_{i}\right] & = & \frac{1}{1+\tau_{\theta}}\left(x_{i}+\tau_{\theta}\tau_{\theta}^{-1}t_{i}\right)\\
 & = & \frac{1}{1+\tau_{\theta}}\left(x_{i}+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Observe that
\begin_inset Formula 
\begin{eqnarray*}
\frac{d}{dt_{i}}\left.\mbe\left[\theta_{i}\vert x_{i},t_{i}\right]\right|_{t_{i}=0} & = & \frac{1}{1+\tau_{\theta}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
as expected.
\end_layout

\begin_layout Subsubsection
EB with a tilted posterior
\end_layout

\begin_layout Standard
Now we will fix 
\begin_inset Formula $\alpha$
\end_inset

 and evaluate the EB posterior approximation as a function of 
\begin_inset Formula $t_{i}$
\end_inset

 and 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau^{-1}\right)\\
x_{i}\vert\theta_{i},t_{i} & \sim & \mathcal{N}\left(\theta_{i},1\right)\\
\mbe\left[\theta_{i}\vert x_{i},t_{i},\alpha\right] & = & \frac{1}{1+\tau}\left(x_{i}+\tau\left(\alpha+\tau^{-1}t_{i}\right)\right)\\
 & = & \frac{1}{1+\tau}\left(x_{i}+\tau\alpha+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So that
\begin_inset Formula 
\begin{eqnarray*}
m & := & \mbe\left[\theta_{i}\vert\alpha,t_{i},x_{i}\right]\\
\frac{\partial m}{\partial t_{i}} & = & \frac{1}{1+\tau}\\
\frac{\partial m}{\partial\alpha} & = & \frac{\tau}{1+\tau}=\tau_{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Formula $\hat{\alpha}$
\end_inset

 as a function of 
\begin_inset Formula $t_{i}$
\end_inset


\end_layout

\begin_layout Standard
Next, we see that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\alpha\vert t_{i} & \sim & \mathcal{N}\left(\tau_{\alpha}^{-1}t_{i},\tau_{\alpha}^{-1}\right)\\
x_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha+\tau^{-1}t_{i},\tau_{x}^{-1}\right)\Rightarrow\\
x_{i}-\tau^{-1}t_{i}\vert\alpha,t_{i} & \sim & \mathcal{N}\left(\alpha,\tau_{x}^{-1}\right)\\
\mbe\left[\alpha\vert x_{i},t_{i}\right] & = & \frac{1}{\tau_{\alpha}+\tau_{x}}\left(\tau_{\alpha}\tau_{\alpha}^{-1}t_{i}+\tau_{x}\left(x_{i}-\tau^{-1}t_{i}\right)\right)\\
 & = & \frac{1}{\tau_{\alpha}+\tau_{x}}\left(t_{i}+\tau_{x}x_{i}-\tau_{x}\tau^{-1}t_{i}\right)\\
 & = & \frac{1}{\tau_{\alpha}+\tau_{x}}\left(\tau_{x}x_{i}+\left(1-\frac{1}{1+\tau}\right)t_{i}\right)\\
 & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}\left(x_{i}+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As a function of 
\begin_inset Formula $\alpha$
\end_inset

, this is maximized at
\begin_inset Formula 
\begin{eqnarray*}
\hat{\alpha} & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}\left(x_{i}+t_{i}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Thus
\begin_inset Formula 
\begin{eqnarray*}
\frac{d\hat{\alpha}}{dt} & = & \frac{\tau_{x}}{\tau_{\alpha}+\tau_{x}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Putting this together, we get that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{dm}{dt} & = & \frac{\partial m}{\partial\alpha}\frac{d\alpha}{dt}+\frac{\partial m}{\partial t}\\
 & = & \frac{\tau_{x}^{2}}{\tau_{\alpha}+\tau_{x}}+\frac{1}{1+\tau}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that the difference is exactly what is expected from the above calculation
 -- the linear response variance adjustment is exact.
\end_layout

\end_body
\end_document
